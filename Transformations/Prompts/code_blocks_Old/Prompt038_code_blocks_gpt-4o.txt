Code Block 1:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);

    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n + threadsPerBlock.x - 1) / threadsPerBlock.x,
                   (n + threadsPerBlock.y - 1) / threadsPerBlock.y,
                   (n + threadsPerBlock.z - 1) / threadsPerBlock.z);

    for (int iter = 0; iter < niter; ++iter) {
        jacobiKernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);
        cudaMemcpy(d_u, d_unew, size, cudaMemcpyDeviceToDevice);
    }

    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobiKernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        int idx = i + n * (j + n * k);
        unew[idx] = 0.125 * (
            u[idx - 1] + u[idx + 1] +
            u[idx - n] + u[idx + n] +
            u[idx - n*n] + u[idx + n*n] +
            u[idx]
        );
    }
}

Code Block 2:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    // Allocate device memory
    cudaMalloc((void**)&d_u, size);
    cudaMalloc((void**)&d_unew, size);

    // Copy data from host to device
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    // Define grid and block dimensions
    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n - 2) / 8 + 1, (n - 2) / 8 + 1, (n - 2) / 8 + 1);

    // Main iteration loop
    for (int iter = 0; iter < niter; ++iter) {
        // Launch the kernel
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);

        // Swap pointers d_u and d_unew
        double* temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    // Copy result back to host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(unew, d_unew, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        unew[i*n*n + j*n + k] = 0.125 * (u[(i-1)*n*n + j*n + k] + u[(i+1)*n*n + j*n + k] + 
                                         u[i*n*n + (j-1)*n + k] + u[i*n*n + (j+1)*n + k] + 
                                         u[i*n*n + j*n + (k-1)] + u[i*n*n + j*n + (k+1)] + 
                                         u[i*n*n + j*n + k]);
    }
}

Code Block 3:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;

    size_t size = n * n * n * sizeof(double);

    // Allocate memory on device
    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);

    // Copy data to device
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n + threadsPerBlock.x - 1) / threadsPerBlock.x,
                   (n + threadsPerBlock.y - 1) / threadsPerBlock.y,
                   (n + threadsPerBlock.z - 1) / threadsPerBlock.z);

    for (int iter = 0; iter < niter; iter++) {
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);

        // Swap pointers
        double* temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    // Copy result back to host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        unew[i * n * n + j * n + k] = 0.125 *
            (u[(i-1) * n * n + j * n + k] + u[(i+1) * n * n + j * n + k] +
             u[i * n * n + (j-1) * n + k] + u[i * n * n + (j+1) * n + k] +
             u[i * n * n + j * n + (k-1)] + u[i * n * n + j * n + (k+1)] +
             u[i * n * n + j * n + k]);
    }
}

Code Block 4:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
    double *d_u, *d_unew;

    size_t size = n * n * n * sizeof(double);

    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);

    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    dim3 blockSize(8, 8, 8); // Adjust as necessary
    dim3 gridSize((n + blockSize.x - 1) / blockSize.x,
                  (n + blockSize.y - 1) / blockSize.y,
                  (n + blockSize.z - 1) / blockSize.z);

    for (int iter = 0; iter < niter; ++iter) {
        jacobi_update<<<gridSize, blockSize>>>(d_u, d_unew, n);
        cudaMemcpy(d_u, d_unew, size, cudaMemcpyDeviceToDevice);
    }

    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_update(double *u, double *unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n - 1 && j < n - 1 && k < n - 1) {
        int idx = i + n * (j + n * k);
        int idx_xm = (i - 1) + n * (j + n * k);
        int idx_xp = (i + 1) + n * (j + n * k);
        int idx_ym = i + n * (j - 1 + n * k);
        int idx_yp = i + n * (j + 1 + n * k);
        int idx_zm = i + n * (j + n * (k - 1));
        int idx_zp = i + n * (j + n * (k + 1));

        unew[idx] = 0.125 * (u[idx_xm] + u[idx_xp] + u[idx_ym] + u[idx_yp] + u[idx_zm] + u[idx_zp]);
    }
}

Code Block 5:
void jacobiParallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);
    
    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);
    
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);
    
    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n + threadsPerBlock.x - 1) / threadsPerBlock.x,
                   (n + threadsPerBlock.y - 1) / threadsPerBlock.y,
                   (n + threadsPerBlock.z - 1) / threadsPerBlock.z);
    
    for (int iter = 0; iter < niter; ++iter) {
        jacobiKernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);
        cudaMemcpy(d_u, d_unew, size, cudaMemcpyDeviceToDevice);
    }
    
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);
    
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobiKernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        int idx = i + n * (j + n * k);
        int idx_xm = (i-1) + n * (j + n * k);
        int idx_xp = (i+1) + n * (j + n * k);
        int idx_ym = i + n * ((j-1) + n * k);
        int idx_yp = i + n * ((j+1) + n * k);
        int idx_zm = i + n * (j + n * (k-1));
        int idx_zp = i + n * (j + n * (k+1));

        unew[idx] = 0.125 * (u[idx_xm] + u[idx_xp] + u[idx_ym] + u[idx_yp] + 
                             u[idx_zm] + u[idx_zp] + u[idx]);
    }
}

Code Block 6:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
    double *d_u, *d_unew;

    // Allocate device memory
    cudaMalloc(&d_u, n * n * n * sizeof(double));
    cudaMalloc(&d_unew, n * n * n * sizeof(double));

    // Copy data from host to device
    cudaMemcpy(d_u, u, n * n * n * sizeof(double), cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, n * n * n * sizeof(double), cudaMemcpyHostToDevice);

    // Define block and grid dimensions
    dim3 dimBlock(8, 8, 8);
    dim3 dimGrid((n + dimBlock.x - 1) / dimBlock.x,
                 (n + dimBlock.y - 1) / dimBlock.y,
                 (n + dimBlock.z - 1) / dimBlock.z);

    for (int iter = 0; iter < niter; iter++) {
        jacobi_kernel<<<dimGrid, dimBlock>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();

        // Swap the pointers for the next iteration
        double* temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    // Copy the result back to the host
    cudaMemcpy(u, d_u, n * n * n * sizeof(double), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double *u, double *unew, int n) {
    // Calculate indexes
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;
    
    if (i < n - 1 && j < n - 1 && k < n - 1) {
        unew[i * n * n + j * n + k] = 0.125 * (u[(i-1) * n * n + j * n + k] + 
                                               u[(i+1) * n * n + j * n + k] + 
                                               u[i * n * n + (j-1) * n + k] + 
                                               u[i * n * n + (j+1) * n + k] + 
                                               u[i * n * n + j * n + (k-1)] + 
                                               u[i * n * n + j * n + (k+1)] + 
                                               u[i * n * n + j * n + k]);
    }
}

Code Block 7:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    // Allocate device memory
    cudaMalloc((void**)&d_u, size);
    cudaMalloc((void**)&d_unew, size);

    // Copy input arrays to GPU
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    // Define grid and block dimensions
    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n + threadsPerBlock.x - 1) / threadsPerBlock.x,
                   (n + threadsPerBlock.y - 1) / threadsPerBlock.y,
                   (n + threadsPerBlock.z - 1) / threadsPerBlock.z);

    for (int iter = 0; iter < niter; ++iter) {
        // Launch kernel
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);

        // Swap pointers
        double* temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    // Copy result back to host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n - 1 && j < n - 1 && k < n - 1) {
        int idx = i + j * n + k * n * n;
        int im1 = (i - 1) + j * n + k * n * n;
        int ip1 = (i + 1) + j * n + k * n * n;
        int jm1 = i + (j - 1) * n + k * n * n;
        int jp1 = i + (j + 1) * n + k * n * n;
        int km1 = i + j * n + (k - 1) * n * n;
        int kp1 = i + j * n + (k + 1) * n * n;

        unew[idx] = 0.125 * (u[im1] + u[ip1] + u[jm1] + u[jp1] + u[km1] + u[kp1] + u[idx]);
    }
}

Code Block 8:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;

    // Allocate device memory
    cudaMalloc(&d_u, n * n * n * sizeof(double));
    cudaMalloc(&d_unew, n * n * n * sizeof(double));

    // Copy data from host to device
    cudaMemcpy(d_u, u, n * n * n * sizeof(double), cudaMemcpyHostToDevice);

    dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE);
    dim3 gridSize((n + BLOCK_SIZE - 1) / BLOCK_SIZE,
                  (n + BLOCK_SIZE - 1) / BLOCK_SIZE,
                  (n + BLOCK_SIZE - 1) / BLOCK_SIZE);

    for (int iter = 0; iter < niter; ++iter) {
        // Launch the kernel
        jacobi_kernel<<<gridSize, blockSize>>>(d_u, d_unew, n);

        // Swap d_u and d_unew pointers
        double* temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    // Copy the result back to host
    cudaMemcpy(u, d_u, n * n * n * sizeof(double), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        unew[i * n * n + j * n + k] = 0.125 * (u[(i-1) * n * n + j * n + k] +
                                               u[(i+1) * n * n + j * n + k] +
                                               u[i * n * n + (j-1) * n + k] +
                                               u[i * n * n + (j+1) * n + k] +
                                               u[i * n * n + j * n + (k-1)] +
                                               u[i * n * n + j * n + (k+1)] +
                                               u[i * n * n + j * n + k]);
    }
}

Code Block 9:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n + threadsPerBlock.x - 3) / (threadsPerBlock.x - 2),   // (n-2)/(blockDim-2) because of halo
                   (n + threadsPerBlock.y - 3) / (threadsPerBlock.y - 2),
                   (n + threadsPerBlock.z - 3) / (threadsPerBlock.z - 2));

    double *d_u, *d_unew;

    cudaMalloc((void**)&d_u, n*n*n * sizeof(double));
    cudaMalloc((void**)&d_unew, n*n*n * sizeof(double));

    cudaMemcpy(d_u, u, n*n*n * sizeof(double), cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, n*n*n * sizeof(double), cudaMemcpyHostToDevice);

    for (int iter = 0; iter < niter; ++iter) {
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();
        // Swap the pointers
        double *temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    cudaMemcpy(u, d_u, n*n*n * sizeof(double), cudaMemcpyDeviceToHost);
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double *u, double *unew, int n) {
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        unew[i + j*n + k*n*n] = 0.125 * (
            u[(i-1) + j*n + k*n*n] +
            u[(i+1) + j*n + k*n*n] +
            u[i + (j-1)*n + k*n*n] +
            u[i + (j+1)*n + k*n*n] +
            u[i + j*n + (k-1)*n*n] +
            u[i + j*n + (k+1)*n*n] +
            u[i + j*n + k*n*n]);
    }
}

Code Block 10:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    // Allocate memory on the GPU
    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);

    // Copy data from host to device
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(8, 8, 8); // Adjust based on your GPU capabilities
    dim3 numBlocks((n + threadsPerBlock.x - 3) / (threadsPerBlock.x - 2), 
                   (n + threadsPerBlock.y - 3) / (threadsPerBlock.y - 2), 
                   (n + threadsPerBlock.z - 3) / (threadsPerBlock.z - 2));

    for (int iter = 0; iter < niter; ++iter) {
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);

        // Swap pointers
        double* temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    // Copy result back to host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    // Free GPU memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        int idx = i*n*n + j*n + k;
        unew[idx] = 0.125 * (u[(i-1)*n*n + j*n + k] + u[(i+1)*n*n + j*n + k] + 
                             u[i*n*n + (j-1)*n + k] + u[i*n*n + (j+1)*n + k] + 
                             u[i*n*n + j*n + (k-1)] + u[i*n*n + j*n + (k+1)] + 
                             u[idx]);
    }
}

