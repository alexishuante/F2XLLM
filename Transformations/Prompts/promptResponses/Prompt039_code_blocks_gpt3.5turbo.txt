Code Block 1:
void spmv_cuda(int n, int nnz, float *val, int *row, int *col, float *x, float *y)
{
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;
    
    cudaMalloc(&d_val, nnz * sizeof(float));
    cudaMalloc(&d_x, n * sizeof(float));
    cudaMalloc(&d_y, n * sizeof(float));
    cudaMalloc(&d_row, (n + 1) * sizeof(int));
    cudaMalloc(&d_col, nnz * sizeof(int));
    
    cudaMemcpy(d_val, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_y, y, n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_row, row, (n + 1) * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_col, col, nnz * sizeof(int), cudaMemcpyHostToDevice);
    
    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;
    
    spmv_parallel<<<numBlocks, blockSize>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);
    
    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);
    
    cudaFree(d_val);
    cudaFree(d_x);
    cudaFree(d_y);
    cudaFree(d_row);
    cudaFree(d_col);
}

__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i < n)
    {
        y[i] = 0.0;
        for (int j = row[i]; j < row[i + 1]; j++)
        {
            y[i] += val[j] * x[col[j]];
        }
    }
}

Code Block 2:
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
  
    if (i < n) {
        y[i] = 0.0;
        for (int j = row[i]; j < row[i + 1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

Code Block 3:
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i < n) {
        y[i] = 0.0f;
        for (int j = row[i]; j < row[i+1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

Code Block 4:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;

    cudaMalloc((void**)&d_val, nnz * sizeof(float));
    cudaMalloc((void**)&d_x, n * sizeof(float));
    cudaMalloc((void**)&d_y, n * sizeof(float));
    cudaMalloc((void**)&d_row, (n+1) * sizeof(int));
    cudaMalloc((void**)&d_col, nnz * sizeof(int));

    cudaMemcpy(d_val, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_y, y, n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_row, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_col, col, nnz * sizeof(int), cudaMemcpyHostToDevice);

    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;

    spmv_kernel<<<numBlocks, blockSize>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);

    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);

    cudaFree(d_val);
    cudaFree(d_x);
    cudaFree(d_y);
    cudaFree(d_row);
    cudaFree(d_col);
}

__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i < n) {
        y[i] = 0.0f;
        for (int j = row[i]; j < row[i+1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

Code Block 5:
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i < n) {
        y[i] = 0.0;
        for (int j = row[i]; j < row[i+1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

Code Block 6:
void spmv(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;
    
    cudaMalloc((void **)&d_val, nnz * sizeof(float));
    cudaMalloc((void **)&d_row, (n + 1) * sizeof(int));
    cudaMalloc((void **)&d_col, nnz * sizeof(int));
    cudaMalloc((void **)&d_x, n * sizeof(float));
    cudaMalloc((void **)&d_y, n * sizeof(float));
    
    cudaMemcpy(d_val, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_row, row, (n + 1) * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_col, col, nnz * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);
    
    int block_size = 256;
    int grid_size = (n + block_size - 1) / block_size;
    
    spmv_parallel<<<grid_size, block_size>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);
    
    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);
    
    cudaFree(d_val);
    cudaFree(d_row);
    cudaFree(d_col);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i < n) {
        y[i] = 0.0;
        for (int j = row[i]; j < row[i + 1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

Code Block 7:
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    
    if (i < n) {
        y[i] = 0.0f;
        for (int j = row[i]; j < row[i + 1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

Code Block 8:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;

    cudaMalloc(&d_val, nnz * sizeof(float));
    cudaMalloc(&d_row, (n+1) * sizeof(int));
    cudaMalloc(&d_col, nnz * sizeof(int));
    cudaMalloc(&d_x, n * sizeof(float));
    cudaMalloc(&d_y, n * sizeof(float));

    cudaMemcpy(d_val, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_row, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_col, col, nnz * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);

    int block_size = 256;
    int grid_size = (n + block_size - 1) / block_size;
    spmv_kernel<<<grid_size, block_size>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);

    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);

    cudaFree(d_val);
    cudaFree(d_row);
    cudaFree(d_col);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = 0.0;
        for (int j = row[i]; j < row[i+1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

Code Block 9:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;

    cudaMalloc(&d_val, nnz * sizeof(float));
    cudaMalloc(&d_row, (n + 1) * sizeof(int));
    cudaMalloc(&d_col, nnz * sizeof(int));
    cudaMalloc(&d_x, n * sizeof(float));
    cudaMalloc(&d_y, n * sizeof(float));

    cudaMemcpy(d_val, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_row, row, (n + 1) * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_col, col, nnz * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);

    int block_size = 256;
    int grid_size = (n + block_size - 1) / block_size;

    spmv_kernel<<<grid_size, block_size>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);

    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);

    cudaFree(d_val);
    cudaFree(d_row);
    cudaFree(d_col);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i < n) {
        y[i] = 0.0;
        for (int j = row[i]; j < row[i + 1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

Code Block 10:
void spmv_parallel(int n, int nnz, float *h_val, int *h_row, int *h_col, float *h_x, float *h_y) {
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;

    cudaMalloc((void**)&d_val, nnz * sizeof(float));
    cudaMalloc((void**)&d_row, (n + 1) * sizeof(int));
    cudaMalloc((void**)&d_col, nnz * sizeof(int));
    cudaMalloc((void**)&d_x, n * sizeof(float));
    cudaMalloc((void**)&d_y, n * sizeof(float));

    cudaMemcpy(d_val, h_val, nnz * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_row, h_row, (n + 1) * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_col, h_col, nnz * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, h_x, n * sizeof(float), cudaMemcpyHostToDevice);

    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;

    spmv_kernel<<<numBlocks, blockSize>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);

    cudaMemcpy(h_y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);

    cudaFree(d_val);
    cudaFree(d_row);
    cudaFree(d_col);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < n) {
        y[tid] = 0.0;
        for (int j = row[tid]; j < row[tid + 1]; j++) {
            y[tid] += val[j] * x[col[j]];
        }
    }
}

