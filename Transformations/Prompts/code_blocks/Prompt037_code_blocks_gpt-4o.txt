Code Block 1:
void jacobi_parallel(double* u, double* unew, int n, int niter, dim3 threadsPerBlock, dim3 numBlocks) {
    double *d_u, *d_unew;

    // Allocate memory on the device
    cudaMalloc(&d_u, n * n * n * sizeof(double));
    cudaMalloc(&d_unew, n * n * n * sizeof(double));

    // Copy the input arrays from the host to the device
    cudaMemcpy(d_u, u, n * n * n * sizeof(double), cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, n * n * n * sizeof(double), cudaMemcpyHostToDevice);

    // Perform Jacobi iterations
    for (int iter = 0; iter < niter; ++iter) {
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();
        cudaMemcpy(d_u, d_unew, n * n * n * sizeof(double), cudaMemcpyDeviceToDevice);
    }

    // Copy the final result back to the host
    cudaMemcpy(u, d_u, n * n * n * sizeof(double), cudaMemcpyDeviceToHost);

    // Free the device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    int k = blockIdx.z * blockDim.z + threadIdx.z;

    if (i >= 2 && i < n-1 && j >= 2 && j < n-1 && k >= 2 && k < n-1) {
        unew[(i * n + j) * n + k] = 0.125 * (u[((i-1) * n + j) * n + k] + 
                                             u[((i+1) * n + j) * n + k] + 
                                             u[(i * n + (j-1)) * n + k] + 
                                             u[(i * n + (j+1)) * n + k] +
                                             u[(i * n + j) * n + (k-1)] + 
                                             u[(i * n + j) * n + (k+1)] + 
                                             u[(i * n + j) * n + k]);
    }
}

Code Block 2:
void jacobi_parallel(double *u, double *unew, int n, int niter, int blockDimX, int blockDimY, int blockDimZ) {
    // Allocate device memory
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);

    // Copy data from host to device
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    // Define the block and grid dimensions
    dim3 block(blockDimX, blockDimY, blockDimZ);
    dim3 grid((n + blockDimX - 2) / blockDimX, (n + blockDimY - 2) / blockDimY, (n + blockDimZ - 2) / blockDimZ);

    for (int iter = 0; iter < niter; iter++) {
        jacobiKernel<<<grid, block>>>(d_u, d_unew, n);

        // Swap pointers d_u and d_unew
        double *tmp = d_u;
        d_u = d_unew;
        d_unew = tmp;
    }

    // Copy result back to host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobiKernel(double *u, double *unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        unew[i*n*n + j*n + k] = 0.125 * (u[(i-1)*n*n + j*n + k] + u[(i+1)*n*n + j*n + k]
                                       + u[i*n*n + (j-1)*n + k] + u[i*n*n + (j+1)*n + k]
                                       + u[i*n*n + j*n + (k-1)] + u[i*n*n + j*n + (k+1)]
                                       + u[i*n*n + j*n + k]);
    }
}

Code Block 3:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    // Allocate device memory
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);

    // Copy initial data from host to device
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    // Define block and grid size
    dim3 blockSize(8, 8, 8);
    dim3 gridSize((n + 7) / 8, (n + 7) / 8, (n + 7) / 8);

    for (int iter = 0; iter < niter; ++iter) {
        // Launch the kernel
        jacobiKernel<<<gridSize, blockSize>>>(d_u, d_unew, n);

        // Synchronize to ensure all threads have completed
        cudaDeviceSynchronize();

        // Swap the pointers
        double* temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    // Copy the result back to host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobiKernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        int idx = i + j*n + k*n*n;
        unew[idx] = 0.125 * (
                      u[idx - 1] + u[idx + 1] +
                      u[idx - n] + u[idx + n] +
                      u[idx - n*n] + u[idx + n*n] +
                      u[idx]);
    }
}

Code Block 4:
void checkCudaErrors(cudaError_t err) {
    if (err != cudaSuccess) {
        std::cerr << "CUDA Error: " << cudaGetErrorString(err) << std::endl;
        exit(err);
    }
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i >= 1 && i < n-1 && j >= 1 && j < n-1 && k >= 1 && k < n-1) {
        unew[IDX(i, j, k, n)] = 0.125 * (u[IDX(i-1, j, k, n)] + u[IDX(i+1, j, k, n)] +
                                         u[IDX(i, j-1, k, n)] + u[IDX(i, j+1, k, n)] +
                                         u[IDX(i, j, k-1, n)] + u[IDX(i, j, k+1, n)] +
                                         u[IDX(i, j, k, n)]);
    }
}

Code Block 5:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);
    
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    dim3 blockDim(8, 8, 8); // You might want to tweak this depending on your GPU capabilities
    dim3 gridDim((n + blockDim.x - 1) / blockDim.x,
                 (n + blockDim.y - 1) / blockDim.y,
                 (n + blockDim.z - 1) / blockDim.z);

    for (int iter = 0; iter < niter; iter++) {
        jacobi_kernel<<<gridDim, blockDim>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();
        cudaMemcpy(d_u, d_unew, size, cudaMemcpyDeviceToDevice);
    }

    cudaMemcpy(u, d_unew, size, cudaMemcpyDeviceToHost);

    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__
void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n - 1 && j < n - 1 && k < n - 1) {
        unew[i * n * n + j * n + k] = 0.125 * (
            u[(i - 1) * n * n + j * n + k] +
            u[(i + 1) * n * n + j * n + k] +
            u[i * n * n + (j - 1) * n + k] +
            u[i * n * n + (j + 1) * n + k] +
            u[i * n * n + j * n + (k - 1)] +
            u[i * n * n + j * n + (k + 1)] +
            u[i * n * n + j * n + k]
        );
    }
}

Code Block 6:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    size_t size = n * n * n * sizeof(double);
    double *d_u, *d_unew;

    // Allocate device memory
    cudaMalloc((void**)&d_u, size);
    cudaMalloc((void**)&d_unew, size);

    // Copy data from host to device
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);

    // Define grid and block dimensions
    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n + threadsPerBlock.x - 1) / threadsPerBlock.x,
                   (n + threadsPerBlock.y - 1) / threadsPerBlock.y,
                   (n + threadsPerBlock.z - 1) / threadsPerBlock.z);

    for (int iter = 0; iter < niter; ++iter) {
        // Launch kernel
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);

        // Swap pointers
        double* tmp = d_u;
        d_u = d_unew;
        d_unew = tmp;
    }

    // Copy result from device to host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n - 1 && j < n - 1 && k < n - 1) {
        unew[i + n * (j + n * k)] = 0.125 * (
            u[(i-1) + n * (j + n * k)] + u[(i+1) + n * (j + n * k)] +
            u[i + n * ((j-1) + n * k)] + u[i + n * ((j+1) + n * k)] +
            u[i + n * (j + n * (k-1))] + u[i + n * (j + n * (k+1))] +
            u[i + n * (j + n * k)]
        );
    }
}

Code Block 7:
void jacobi_parallel(double *h_u, double *h_unew, int n, int niter) {
    // Allocate device memory
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);
    cudaMalloc((void**)&d_u, size);
    cudaMalloc((void**)&d_unew, size);
    
    // Copy data from host to device
    cudaMemcpy(d_u, h_u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, h_unew, size, cudaMemcpyHostToDevice);

    // Set up execution configuration
    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n + threadsPerBlock.x - 1) / threadsPerBlock.x,
                   (n + threadsPerBlock.y - 1) / threadsPerBlock.y,
                   (n + threadsPerBlock.z - 1) / threadsPerBlock.z);

    // Execute kernel and iterate
    for (int iter = 0; iter < niter; ++iter) {
        jacobiKernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();  // Ensure all threads are done
        std::swap(d_u, d_unew);   // Swap pointers for next iteration
    }
    
    // Copy the result back to host
    cudaMemcpy(h_u, d_u, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__
void jacobiKernel(double* u, double* unew, int n) {
    int ix = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int iy = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int iz = blockIdx.z * blockDim.z + threadIdx.z + 1;
    
    if (ix < n - 1 && iy < n - 1 && iz < n - 1) {
        int idx = iz * n * n + iy * n + ix;
        unew[idx] = 0.125 * (
            u[idx - 1] + u[idx + 1] + 
            u[idx - n] + u[idx + n] + 
            u[idx - n * n] + u[idx + n * n] + u[idx]
        );
    }
}

Code Block 8:
int IDX(int i, int j, int k, int n) {
    return (i * n * n) + (j * n) + k;
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    int k = blockIdx.z * blockDim.z + threadIdx.z;

    if (i >= 1 && i < n-1 && j >= 1 && j < n-1 && k >= 1 && k < n-1) {
        unew[IDX(i,j,k,n)] = 0.125 * (u[IDX(i-1,j,k,n)] + u[IDX(i+1,j,k,n)] + 
                                      u[IDX(i,j-1,k,n)] + u[IDX(i,j+1,k,n)] + 
                                      u[IDX(i,j,k-1,n)] + u[IDX(i,j,k+1,n)] + 
                                      u[IDX(i,j,k,n)]);
    }
}

Code Block 9:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    // Allocate device memory
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);
    cudaMalloc((void**)&d_u, size);
    cudaMalloc((void**)&d_unew, size);

    // Copy the initial data from the host to the device
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    // Define the block and grid sizes
    int blockSize = 8;
    dim3 dimBlock(blockSize, blockSize, blockSize);
    dim3 dimGrid((n + blockSize - 1) / blockSize, (n + blockSize - 1) / blockSize, (n + blockSize - 1) / blockSize);

    for (int iter = 0; iter < niter; iter++) {
        // Launch the kernel
        jacobiKernel<<<dimGrid, dimBlock>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();

        // Swap pointers
        double *temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    // Copy the result back from the device to the host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    // Free the device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__
void jacobiKernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1; // Offset by 1 to skip the boundary
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n - 1 && j < n - 1 && k < n - 1) {
        unew[i*(n*n) + j*n + k] = 0.125 * (u[(i-1)*(n*n) + j*n + k] +
                                           u[(i+1)*(n*n) + j*n + k] +
                                           u[i*(n*n) + (j-1)*n + k] +
                                           u[i*(n*n) + (j+1)*n + k] +
                                           u[i*(n*n) + j*n + (k-1)] +
                                           u[i*(n*n) + j*n + (k+1)] +
                                           u[i*(n*n) + j*n + k]);
    }
}

Code Block 10:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    // Allocate memory on the device
    cudaMalloc((void**)&d_u, size);
    cudaMalloc((void**)&d_unew, size);

    // Copy initial arrays from host to device
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(16, 16, 1);
    dim3 numBlocks((n + threadsPerBlock.x - 1) / threadsPerBlock.x, 
                   (n + threadsPerBlock.y - 1) / threadsPerBlock.y, 
                   (n + threadsPerBlock.z - 1) / threadsPerBlock.z);

    // Iterate the Jacobi kernel
    for (int iter = 0; iter < niter; ++iter) {
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();

        // Swap the pointers
        double *tmp = d_u;
        d_u = d_unew;
        d_unew = tmp;
    }

    // Copy result back to host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        unew[IDX(i, j, k, n)] = 0.125 * (u[IDX(i-1, j, k, n)] + u[IDX(i+1, j, k, n)] 
                                        + u[IDX(i, j-1, k, n)] + u[IDX(i, j+1, k, n)] 
                                        + u[IDX(i, j, k-1, n)] + u[IDX(i, j, k+1, n)] 
                                        + u[IDX(i, j, k, n)]);
    }
}

