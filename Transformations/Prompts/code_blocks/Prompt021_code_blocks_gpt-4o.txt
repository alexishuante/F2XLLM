Code Block 1:
void saxpy_parallel(int n, float a, const float *x, float *y) {
    float *d_x, *d_y;
    
    // Allocate device memory
    hipMalloc((void**)(&d_x), n * sizeof(float));
    hipMalloc((void**)(&d_y), n * sizeof(float));

    // Copy vectors from host memory to device memory
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_y, y, n * sizeof(float), hipMemcpyHostToDevice);

    // Define the number of blocks and threads per block
    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;

    // Launch HIP kernel to compute SAXPY
    hipLaunchKernelGGL(saxpy_kernel, dim3(numBlocks), dim3(blockSize), 0, 0, n, a, d_x, d_y);

    // Wait for GPU to finish before accessing on host
    hipDeviceSynchronize();

    // Copy result back to host memory
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_x);
    hipFree(d_y);
}

__global__
void saxpy_kernel(int n, float a, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

Code Block 2:
void saxpy_omp(int n, float a, const float* x, float* y) {
    #pragma omp parallel for
    for (int i = 0; i < n; ++i) {
        y[i] = a * x[i] + y[i];
    }
}

__global__ void saxpy_kernel(int n, float a, float* x, float* y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

Code Block 3:
void saxpy(int n, float a, float* x, float* y) {
    // Set device (optional, for multiple GPUs)
    hipSetDevice(0);

    // Allocate device memory
    float *d_x, *d_y;
    hipMalloc(&d_x, n * sizeof(float));
    hipMalloc(&d_y, n * sizeof(float));

    // Copy data from host to device
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_y, y, n * sizeof(float), hipMemcpyHostToDevice);

    // Launch kernel with enough threads/blocks to cover all elements
    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;
    hipLaunchKernelGGL(saxpy_parallel, dim3(numBlocks), dim3(blockSize), 0, 0, n, a, d_x, d_y);

    // Copy results from device back to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_x);
    hipFree(d_y);
}

__global__
void saxpy_parallel(int n, float a, float* x, float* y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

Code Block 4:
void saxpy_parallel(int n, float a, float* x, float* y) {
    const int blockSize = 256; // The number of threads per block
    const int numBlocks = (n + blockSize - 1) / blockSize; // Calculate number of blocks

    // Allocate device memory
    float *d_x, *d_y;
    hipMalloc(&d_x, n * sizeof(float));
    hipMalloc(&d_y, n * sizeof(float));

    // Copy data from host to device
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_y, y, n * sizeof(float), hipMemcpyHostToDevice);

    // Launch the kernel
    hipLaunchKernelGGL(saxpy_kernel, dim3(numBlocks), dim3(blockSize), 0, 0, n, a, d_x, d_y);

    // Copy result back to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void saxpy_kernel(int n, float a, float* x, float* y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

Code Block 5:
void saxpy_parallel(int n, float a, const float *x, float *y) {
    // Define device pointers
    float *d_x, *d_y;

    // Allocate device memory
    hipMalloc((void**)&d_x, n * sizeof(float));
    hipMalloc((void**)&d_y, n * sizeof(float));

    // Copy data from host to device
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_y, y, n * sizeof(float), hipMemcpyHostToDevice);

    // Define the number of threads per block and number of blocks
    int blockSize = 256; // Number of threads per block
    int numBlocks = (n + blockSize - 1) / blockSize;

    // Launch the HIP kernel
    hipLaunchKernelGGL(saxpy_kernel,
                       dim3(numBlocks),
                       dim3(blockSize),
                       0, 0,   // Shared memory size and stream
                       n, a, d_x, d_y);

    // Copy result back from device to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void saxpy_kernel(int n, float a, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

Code Block 6:
void saxpy_parallel(int n, float a, const float *x, float *y) {
    // Define the number of threads per block and the number of blocks
    int blockSize = 256;  // You can adjust this for better performance
    int numBlocks = (n + blockSize - 1) / blockSize;
    
    // Allocate device memory
    float *d_x, *d_y;
    hipMalloc((void**)&d_x, n * sizeof(float));
    hipMalloc((void**)&d_y, n * sizeof(float));
    
    // Copy data from host to device
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_y, y, n * sizeof(float), hipMemcpyHostToDevice);
    
    // Launch the kernel
    hipLaunchKernelGGL(saxpy_kernel, numBlocks, blockSize, 0, 0, n, a, d_x, d_y);
    
    // Copy result back to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);
    
    // Free device memory
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void saxpy_kernel(int n, float a, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

Code Block 7:
void saxpy_parallel(int n, float a, const float *x, float *y) {
    float *d_x, *d_y;
    
    // Allocate device memory
    hipMalloc(&d_x, n * sizeof(float));
    hipMalloc(&d_y, n * sizeof(float));
    
    // Copy data to device
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_y, y, n * sizeof(float), hipMemcpyHostToDevice);
    
    // Define block and grid sizes
    int blockSize = 256; // Adjust block size to your needs
    int gridSize = (n + blockSize - 1) / blockSize;
    
    // Launch the HIP kernel
    hipLaunchKernelGGL(saxpy, dim3(gridSize), dim3(blockSize), 0, 0, n, a, d_x, d_y);
    
    // Copy result back to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);
    
    // Free device memory
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void saxpy(int n, float a, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

Code Block 8:
void saxpy_parallel(int n, float a, const float* x, float* y) {
    float *d_x, *d_y;
    size_t size = n * sizeof(float);

    // Allocate device memory
    hipMalloc(&d_x, size);
    hipMalloc(&d_y, size);

    // Copy data from host to device
    hipMemcpy(d_x, x, size, hipMemcpyHostToDevice);
    hipMemcpy(d_y, y, size, hipMemcpyHostToDevice);

    // Launch the kernel
    int blockSize = 256; // You can experiment with this value
    int numBlocks = (n + blockSize - 1) / blockSize;
    hipLaunchKernelGGL(saxpy_kernel, dim3(numBlocks), dim3(blockSize), 0, 0, n, a, d_x, d_y);

    // Copy result back to host
    hipMemcpy(y, d_y, size, hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void saxpy_kernel(int n, float a, const float* x, float* y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

Code Block 9:
void saxpy_parallel(int n, float a, const float *x, float *y) {
    float *d_x, *d_y;

    // Allocate device memory
    hipMalloc(&d_x, n * sizeof(float));
    hipMalloc(&d_y, n * sizeof(float));

    // Copy data from host to device
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_y, y, n * sizeof(float), hipMemcpyHostToDevice);

    // Determine block and grid sizes
    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;

    // Launch the kernel
    hipLaunchKernelGGL(saxpyKernel, numBlocks, blockSize, 0, 0, n, a, d_x, d_y);

    // Copy result back to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void saxpyKernel(int n, float a, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

Code Block 10:
void saxpy_parallel(int n, float a, const float *x, float *y) {
    // Define the number of threads per block and number of blocks per grid
    const int threadsPerBlock = 256;
    const int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;

    // Allocate GPU memory
    float *d_x = nullptr;
    float *d_y = nullptr;
    hipMalloc((void**)&d_x, n * sizeof(float));
    hipMalloc((void**)&d_y, n * sizeof(float));

    // Copy inputs to GPU
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_y, y, n * sizeof(float), hipMemcpyHostToDevice);

    // Launch the HIP kernel
    saxpy<<<blocksPerGrid, threadsPerBlock>>>(n, a, d_x, d_y);

    // Check for kernel launch errors
    hipError_t err = hipGetLastError();
    if (err != hipSuccess) {
        std::cerr << "HIP kernel launch error: " << hipGetErrorString(err) << std::endl;
        return;
    }

    // Copy result back to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free GPU memory
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void saxpy(int n, float a, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}

