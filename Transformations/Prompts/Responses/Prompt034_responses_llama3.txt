Output 1:
Here is the equivalent code in C and CUDA:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
CUDA:
__global__ void gemm_parallel_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
You can call the CUDA kernel like this:
int blockSize = 16;
int numBlocks = (m + blockSize - 1) / blockSize;
gemm_parallel_kernel<<<dim3(numBlocks, numBlocks), dim3(blockSize, blockSize)>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
Note that the CUDA kernel uses a 2D block and grid structure to parallelize the computation across multiple threads and blocks. The blockSize variable determines the number of threads per block, and the numBlocks variable determines the number of blocks in the grid.
Also, note that the CUDA kernel uses shared memory to store the temporary results, which can improve performance by reducing global memory access. However, this requires manual memory management, which can be error-prone. Alternatively, you can use the cudaMemcpy function to copy the data to and from the device, but this may incur additional memory transfer overhead.


Output 2:
Here is the equivalent function implemented in C and CUDA:
C Version:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
CUDA Version:
__global__ void gemm_parallel_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int j = blockIdx.x * blockDim.x + threadIdx.x;
  int i = blockIdx.y * blockDim.y + threadIdx.y;
  if (j < n && i < m) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
To call the CUDA kernel, you would need to setup the grid and block dimensions, and pass the pointers to the device memory:
int blockSize = 16;
int numBlocks = (m + blockSize - 1) / blockSize;
gemm_parallel_kernel<<<dim3(numBlocks, numBlocks), dim3(blockSize, blockSize)>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
Note that in the CUDA version, I assumed a 2D block and grid layout to take advantage of the 2D nature of the GEMM operation. You may need to adjust the block and grid sizes based on your specific hardware and problem size.


Output 3:
Here is the equivalent function in C and CUDA, and the subroutine in Fortran and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
CUDA:
__global__ void gemm_parallel_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran with OpenACC:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$ACC END PARALLEL LOOP
end subroutine gemm_parallel
Note that in the CUDA version, I've assumed a 2D block and grid layout to parallelize the computation. You'll need to set up the block and grid dimensions appropriately when launching the kernel. Also, the CUDA kernel uses a __global__ declaration to indicate that it's a kernel that runs on the device.


Output 4:
Here is the equivalent function in C and CUDA, and the subroutine in Fortran and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
CUDA:
__global__ void gemm_parallel_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int j = blockIdx.x * blockDim.x + threadIdx.x;
  int i = blockIdx.y * blockDim.y + threadIdx.y;
  if (j < n && i < m) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran with OpenACC:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$ACC END PARALLEL LOOP
end subroutine gemm_parallel
Note that the CUDA kernel is launched on the host code as follows:
int blockSize = 16;
int numBlocks = (n + blockSize - 1) / blockSize;
gemm_parallel_kernel<<<numBlocks, blockSize>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
Also, the array c is assumed to be allocated on the device in the CUDA version. If it's not, you need to allocate it on the device and copy the data from the host to the device before launching the kernel.


Output 5:
Here is the equivalent function in C and CUDA:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
CUDA:
__global__ void gemm_parallel_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Note that the CUDA kernel is launched with a 2D grid of blocks, where each block is responsible for a subset of the matrix elements. The blockIdx and threadIdx variables are used to compute the global indices i and j.
To launch the kernel, you would need to configure the grid and block dimensions, and pass the appropriate pointers to the device memory:
int blockSize = 16;
int numBlocks = (m + blockSize - 1) / blockSize;
dim3 gridDim(numBlocks, numBlocks, 1);
dim3 blockDim(blockSize, blockSize, 1);

double *a_d, *b_d, *c_d;
cudaMalloc((void **)&a_d, lda * k * sizeof(double));
cudaMalloc((void **)&b_d, ldb * n * sizeof(double));
cudaMalloc((void **)&c_d, ldc * n * sizeof(double));

gemm_parallel_kernel<<<gridDim, blockDim>>>(m, n, k, alpha, a_d, lda, b_d, ldb, beta, c_d, ldc);
I hope this helps! Let me know if you have any questions.


Output 6:
Here is the equivalent function in C and CUDA, and also in Fortran with OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
CUDA:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran with OpenACC:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$ACC END PARALLEL LOOP
end subroutine gemm_parallel
Note that in the CUDA version, I assumed a block size of 16x16, you may need to adjust this based on your specific use case. Also, the CUDA version uses a 2D block and grid to cover the entire matrix, you may need to add error checking code to handle cases where the matrix size is not a multiple of the block size.


Output 7:
Here is the equivalent code in C and CUDA, and also in CUDA with cuBLAS:
C Code:
#include <stdio.h>

void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
CUDA Code:
#include <stdio.h>

__global__ void gemm_parallel_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int j = blockIdx.x * blockDim.x + threadIdx.x;
  int i = blockIdx.y * blockDim.y + threadIdx.y;
  if (j < n && i < m) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = ceilf(max(m, n) / (float)blockSize);
  gemm_parallel_kernel<<<dim3(numBlocks, numBlocks), dim3(blockSize, blockSize)>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
}
cuBLAS Code:
#include <cuda_runtime.h>
#include <cublas_v2.h>

void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  cublasHandle_t handle;
  cublasCreate(&handle);
  cublasDgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc);
  cublasDestroy(handle);
}
Note that in the CUDA code, I assumed a block size of 16x16, but you may need to adjust this depending on your specific hardware and use case. Also, in the cuBLAS code, I used the CUBLAS_OP_N option, which means that the matrices are not transposed. If your matrices are transposed, you may need to use CUBLAS_OP_T instead.


Output 8:
Here is the equivalent code in C and CUDA:
C Code:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
CUDA Code:
__global__ void gemm_parallel_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int j = blockIdx.x * blockDim.x + threadIdx.x;
  int i = blockIdx.y * blockDim.y + threadIdx.y;
  if (j < n && i < m) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Host Code:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = (m + blockSize - 1) / blockSize;
  gemm_parallel_kernel<<<dim3(numBlocks, numBlocks), dim3(blockSize, blockSize)>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
}
Note: In the CUDA code, I assumed a 2D block and grid structure to parallelize the computation. The block size is set to 16, but you can adjust it based on your specific hardware and performance requirements.
Also, the gemm_parallel function in C and the gemm_parallel_kernel function in CUDA are equivalent to the original Fortran subroutine. The host code in C is responsible for launching the CUDA kernel.


Output 9:
Here are the equivalent functions in C and CUDA, and in C and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
CUDA:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in the CUDA version, I assumed a block size of (16, 16) for the sake of example, but you may need to adjust this depending on your specific use case. Also, in the OpenACC version, I used the routine seq directive to ensure that the routine is executed sequentially on the device, but you may want to adjust this depending on your specific use case as well.


Output 10:
Here is the equivalent function in C and CUDA:
C Version:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
CUDA Version:
__global__ void gemm_parallel_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int j = blockIdx.x * blockDim.x + threadIdx.x;
  int i = blockIdx.y * blockDim.y + threadIdx.y;
  if (j < n && i < m) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
And the CUDA wrapper function:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = (n + blockSize - 1) / blockSize;
  gemm_parallel_kernel<<<dim3(numBlocks, numBlocks), dim3(blockSize, blockSize)>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
  cudaDeviceSynchronize();
}
Note that in the CUDA version, I've used a 2D block and grid to parallelize the computation over the m and n dimensions. The blockSize variable determines the number of threads in each block, and the numBlocks variable determines the number of blocks in each dimension. The cudaDeviceSynchronize() call is used to wait for the kernel to finish execution before returning from the function.


