Output 1:
Here is the equivalent function implemented in C with OpenACC:
#include <stdio.h>
#include <stdlib.h>
#include <openacc.h>

void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed that the real(kind=8) type in Fortran corresponds to double in C. Also, the lda, ldb, and ldc parameters are used to access the 2D arrays a, b, and c in column-major order, as is typical in Fortran.
The OpenACC directives are used to parallelize the loops, with the private clause specifying that each thread should have its own copy of the variables i, j, l, and temp. The reduction clause is used to perform a reduction operation on the temp variable, which is necessary because each thread is computing a partial sum that needs to be combined with the partial sums from other threads.
Keep in mind that this is just one possible implementation, and there may be other ways to optimize the code for performance on your specific hardware.


Output 2:
Here is the equivalent function implemented in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector collapse(2) present(a, b, c)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) in Fortran is equivalent to double in C. Also, I've used the gang vector collapse clause to parallelize the loop over i and j, and the reduction clause to parallelize the summation over l. The present clause ensures that the arrays a, b, and c are present on the accelerator device.


Output 3:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector collapse(2) present(a, b, c)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) in Fortran corresponds to double in C, and adjusted the loop indices and array indexing accordingly. Also, I've used #pragma acc routine seq to indicate that this function should be compiled in sequence (i.e., not parallelized) and #pragma acc parallel loop to parallelize the loops. The gang vector clause is used to specify the parallelization strategy, and collapse(2) is used to collapse the two inner loops into a single parallel loop. Finally, present(a, b, c) is used to indicate that the arrays a, b, and c are present on the device.
Please note that this is a straightforward translation, you may need to adjust the pragmas to suit your specific use case.


Output 4:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector collapse(3) present(a, b, c)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) in Fortran corresponds to double in C, and adjusted the indexing accordingly. Also, I've used gang vector parallelism in OpenACC to parallelize the outer two loops, and reduction clause to parallelize the inner loop.


Output 5:
Here is the equivalent function implemented in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector collapse(2) present(a, b, c)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop seq
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) in Fortran corresponds to double in C. Also, I've used gang vector collapse to parallelize the loop over j and i, and seq for the loop over l to ensure the correctness of the reduction operation.
You can call this function on the host and offload it to the device using OpenACC's #pragma acc data directive.


Output 6:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector collapse(2) private(temp)
  for (int j = 0; j < n; j++) {
    for (int i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (int l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) in Fortran corresponds to double in C. Also, the lda, ldb, and ldc arguments are used to access the 2D arrays a, b, and c in column-major order, as in Fortran.
To compile this code, you'll need to use a compiler that supports OpenACC, such as GCC or PGI. For example:
gcc -fopenacc -o gemm_parallel gemm_parallel.c
This will generate an executable file gemm_parallel that you can run on a GPU or other accelerator device supported by OpenACC.


Output 7:
Here is a possible implementation of the GEMM operation in C using OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector collapse(2) present(a, b, c)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that this implementation uses the gang vector collapse clause to parallelize the outer two loops, and the reduction clause to parallelize the inner loop. The present clause is used to ensure that the arrays a, b, and c are present on the accelerator device.
Also, the #pragma acc routine seq directive is used to indicate that this function should be compiled for the host CPU, and the #pragma acc parallel directive is used to indicate that the function should be executed on the accelerator device.
You can call this function from your Fortran code using the ISO_C_BINDING module, like this:
use iso_c_binding
...
interface
  subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc) bind(C)
    use iso_c_binding
    integer(C_INT), value :: m, n, k, lda, ldb, ldc
    real(C_DOUBLE), value :: alpha, beta
    real(C_DOUBLE) :: a(lda, k), b(ldb, n)
    real(C_DOUBLE) :: c(ldc, n)
  end subroutine gemm_parallel
end interface
...
call gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
Note that the bind(C) attribute is used to specify the C binding for the function, and the iso_c_binding module is used to specify the C types for the arguments.


Output 8:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector collapse(2) present(a, b, c)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed that the real(kind=8) type in Fortran corresponds to double in C. Also, I've used the gang vector collapse clause to parallelize the loop over i and j, and the reduction clause to parallelize the summation over l. The present clause is used to ensure that the arrays a, b, and c are present on the device.
You can call this function from your host code, and the OpenACC runtime will take care of offloading the computation to the device (e.g. GPU).


Output 9:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) in Fortran corresponds to double in C. Also, I've used gang vector parallelism in OpenACC to parallelize the outer loops, and reduction clause to perform the reduction operation on the temp variable.
You can compile this code with a OpenACC-enabled compiler, such as pgcc from the PGI suite, and run it on a GPU or other accelerator device.


Output 10:
Here is the equivalent function in C with OpenACC:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C, arrays are represented as pointers, so we need to explicitly calculate the indices using the lda, ldb, and ldc parameters. Also, the reduction clause is used to specify that the temp variable is a reduction variable, which allows the compiler to correctly parallelize the loop.
To compile this code with OpenACC, you can use a compiler such as GCC or PGI, with the -fopenacc flag. For example:
gcc -fopenacc gemm_parallel.c -o gemm_parallel
This will generate an executable file gemm_parallel that can be run on a GPU or other accelerator device.


