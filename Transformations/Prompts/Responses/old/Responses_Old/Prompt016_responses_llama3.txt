Output 1:
Here is the equivalent C/C++ OpenACC code for the given Fortran OpenACC code:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C/C++, array indices start at 0, whereas in Fortran, they start at 1. Also, the lda, ldb, and ldc parameters are used to access the elements of the arrays a, b, and c, respectively, in a column-major order, as in Fortran.
To compile this code, you'll need to use a C/C++ compiler that supports OpenACC, such as GCC or PGI. You may also need to link against an OpenACC implementation library, such as libacc or libopenacc.


Output 2:
Here is the equivalent C/C++ OpenACC code for the given Fortran OpenACC code that implements a Generalized Matrix multiplication (GEMM) operation:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C/C++, array indices start at 0, so the loop counters start at 0 instead of 1. Also, the array access is done using pointer arithmetic instead of the Fortran style array access.


Output 3:
Here is a version of the code in C/C++ using OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C/C++, arrays are passed to functions as pointers, so we don't need to specify the size of the arrays as we do in Fortran. Also, the collapse(2) clause tells OpenACC to collapse the two nested loops into a single loop, which can improve parallelization efficiency.
I've also replaced the real(kind=8) with float, assuming that you want to use single-precision floating-point numbers. If you need double-precision, you can replace float with double.
You can call this function from your host code like this:
int main() {
  int m = 100, n = 100, k = 100;
  float alpha = 1.0, beta = 0.0;
  float *a = (float *)malloc(m * k * sizeof(float));
  float *b = (float *)malloc(k * n * sizeof(float));
  float *c = (float *)malloc(m * n * sizeof(float));

  // Initialize a, b, and c

  #pragma acc data copyin(a[0:m*k], b[0:k*n]) copy(c[0:m*n])
  {
    gemm_parallel(m, n, k, alpha, a, m, b, k, beta, c, m);
  }

  // Use the result in c

  free(a);
  free(b);
  free(c);
  return 0;
}
This code allocates memory for the matrices a, b, and c, initializes them, and then calls the gemm_parallel function using OpenACC's data directive to copy the data to the accelerator. The copyin clause specifies that the data in a and b is copied to the accelerator, while the copy clause specifies that the data in c is copied back to the host after the computation is complete.


Output 4:
Here is a C/C++ version of the OpenACC code that implements a Generalized Matrix multiplication (GEMM) operation:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C/C++, we use pointers to arrays instead of explicit arrays, and we use the lda and ldb parameters to access the elements of the matrices a and b respectively. Also, the collapse(2) clause is used to collapse the two nested loops into a single loop, and the private clause is used to specify that the variables i, j, l, and temp are private to each thread.
You can call this function from your host code like this:
int main() {
  int m = 1024, n = 1024, k = 1024;
  float alpha = 1.0, beta = 0.0;
  float *a, *b, *c;

  // Allocate memory for matrices
  a = (float *)malloc(m * k * sizeof(float));
  b = (float *)malloc(k * n * sizeof(float));
  c = (float *)malloc(m * n * sizeof(float));

  // Initialize matrices
  // ...

  #pragma acc data copy(a[:m*k], b[:k*n], c[:m*n])
  {
    gemm_parallel(m, n, k, alpha, a, m, b, k, beta, c, m);
  }

  // Deallocate memory
  free(a);
  free(b);
  free(c);

  return 0;
}
This code allocates memory for the matrices a, b, and c, initializes them, and then calls the gemm_parallel function using the #pragma acc data directive to specify that the matrices should be copied to the device. The gemm_parallel function is then executed on the device, and the resulting matrix c is copied back to the host.


Output 5:
Here is the equivalent C/C++ OpenACC code for the provided Fortran OpenACC code that implements a Generalized Matrix multiplication (GEMM) operation:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C/C++, array indices start at 0, whereas in Fortran, they start at 1. Also, the array access is done using pointer arithmetic instead of the Fortran style a(i, l). The #pragma acc routine seq directive is used to indicate that the routine should be compiled in sequential mode, and the #pragma acc parallel loop directive is used to parallelize the loop. The collapse(2) clause is used to collapse the two nested loops into a single parallel loop. The private clause is used to specify that the variables i, j, l, and temp should be privatized for each thread.


Output 6:
Here is a C/C++ version of the OpenACC code for the GEMM operation:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C/C++, we use pointers to arrays instead of arrays themselves, and we also use row-major ordering (i.e., the row index varies fastest). The #pragma acc routine seq directive tells the compiler to generate a sequential version of the routine, and the #pragma acc parallel loop directive parallelizes the loop. The collapse(2) clause tells the compiler to collapse the two inner loops into a single loop, and the private clause specifies the variables that should be privatized for each thread.
Also, note that I've assumed double precision for the floating-point numbers, but you can change it to float or any other type if needed.


Output 7:
Here is the equivalent C/C++ OpenACC code for the given Fortran OpenACC code:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C/C++, array indices start at 0, so the loop counters start at 0 and go up to m-1, n-1, and k-1 respectively. Also, the array accesses are done using the pointer notation, and the row-major order is assumed. The #pragma acc routine seq directive tells the compiler to generate a sequential version of the routine, and the #pragma acc parallel loop directive parallelizes the loop. The collapse(2) clause collapses the two inner loops into a single loop, and the private clause specifies the private variables for each thread.


Output 8:
Here is the equivalent C/C++ OpenACC code for the given Fortran OpenACC code that implements a Generalized Matrix multiplication (GEMM) operation:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C/C++, arrays are 0-indexed, so the loop indices start from 0 instead of 1. Also, the array indices are calculated manually using the lda and ldb parameters, since C/C++ does not have built-in support for 2D arrays like Fortran does.
I used double as the type for the arrays, assuming that's what real(kind=8) corresponds to in your Fortran code. If that's not the case, you can adjust the type accordingly.


Output 9:
Here is the equivalent C/C++ OpenACC code for the given Fortran OpenACC code:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C/C++, array indices start at 0, whereas in Fortran, they start at 1. Also, the array access is done using pointer arithmetic instead of the Fortran style of using indices. The #pragma acc routine seq directive is used to indicate that the routine can be executed sequentially on the accelerator.


Output 10:
Here is a version of the code in C/C++ with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  #pragma acc parallel loop collapse(2) private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) corresponds to float in C/C++. If you need double precision, you can replace float with double. Also, I've used the routine seq directive to ensure that the routine is executed sequentially on the host, and the parallel loop directive to parallelize the loops on the accelerator. The reduction directive is used to perform the reduction operation on the temp variable.


