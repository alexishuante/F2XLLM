Output 1:
Here is a version of the code in C/C++ using OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) is equivalent to float in C/C++. If you need double precision, you can replace float with double. Also, I've used the gang vector parallelism model, which is similar to the parallel do construct in OpenMP.
To compile this code, you'll need to use a compiler that supports OpenACC, such as GCC or PGI. You may also need to link against an accelerator library, depending on your system configuration.
You can call this function from your host code like this:
int main() {
  int m = 1024;
  int n = 1024;
  int k = 1024;
  float alpha = 1.0;
  float beta = 0.0;
  float *a = (float *)malloc(m * k * sizeof(float));
  float *b = (float *)malloc(k * n * sizeof(float));
  float *c = (float *)malloc(m * n * sizeof(float));

  // Initialize a, b, and c

  gemm_parallel(m, n, k, alpha, a, m, b, k, beta, c, m);

  // Use the result in c

  free(a);
  free(b);
  free(c);
  return 0;
}
This code will offload the GEMM operation to an accelerator device, such as a GPU, if one is available. If no accelerator is available, the code will fall back to executing on the host CPU.


Output 2:
Here is the equivalent C/C++ OpenACC code for the given Fortran OpenMP code:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that OpenACC is a directive-based parallel programming model, similar to OpenMP. However, OpenACC is specifically designed for accelerators, such as GPUs, whereas OpenMP can be used on various platforms, including CPUs and accelerators.
In this code, #pragma acc parallel loop is used to parallelize the outer loops, and #pragma acc loop reduction(+:temp) is used to perform a reduction on the temp variable. The gang vector clause is used to specify the parallelization strategy.
Also, note that the array indexing is changed to reflect the C/C++ convention (0-based indexing) instead of Fortran's 1-based indexing.


Output 3:
Here is a version of the code in C/C++ using OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) is equivalent to float in C/C++. If you're using a different type (e.g. double), you'll need to adjust the code accordingly. Also, OpenACC uses gang and vector parallelism, which is different from OpenMP's PARALLEL DO construct.
You can compile this code with an OpenACC-enabled compiler (such as PGI or GCC) and run it on a GPU or other accelerator device.


Output 4:
Here is a version of the code in C/C++ using OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) corresponds to float in C/C++. If you need double precision, you can replace float with double. Also, I've used gang vector parallelism in OpenACC, which is similar to parallel do in OpenMP. The reduction clause is used to perform the sum reduction on the temp variable.
You can compile this code with an OpenACC-enabled compiler, such as PGI or GCC, and run it on a GPU or other accelerator.


Output 5:
Here is the equivalent C/C++ OpenACC code for the Fortran OpenMP code you provided:
#include <accel.h>

void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've used the collapse(2) clause to collapse the two nested loops into a single parallel loop, and the reduction(+:temp) clause to perform a reduction on the temp variable.
Also, I've assumed that the arrays a, b, and c are stored in row-major order, as is typical in C/C++. If your arrays are stored in column-major order, you may need to adjust the indexing accordingly.
Finally, I've used the double type to match the real(kind=8) type in the Fortran code. If you need to use a different type (e.g. float), you can modify the code accordingly.


Output 6:
Here is the equivalent C/C++ OpenACC code for the given Fortran OpenMP code:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop seq
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that OpenACC is designed for parallelization on accelerators like GPUs, whereas OpenMP is a shared-memory parallel programming model. Therefore, the code structure and syntax are slightly different. The #pragma acc routine seq directive tells the compiler to generate a sequential version of the routine for the host, while #pragma acc parallel loop initiates parallelization on the accelerator. The gang vector clause specifies the parallelization strategy across gangs and vectors ( threads ). The private clause ensures that each thread has its own copy of the variables. The #pragma acc loop seq directive is used for the innermost loop to avoid nested parallelism.


Output 7:
Here is a version of the code in C/C++ using OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop seq
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) is equivalent to float in C/C++. If you're using a different type (e.g. double), you'll need to adjust the code accordingly. Also, I've used gang vector parallelism in OpenACC, which is similar to the parallelization done by OpenMP in the original Fortran code.
You can call this function from your host code like this:
int main() {
  int m = 100, n = 100, k = 100;
  float alpha = 1.0, beta = 1.0;
  float *a = (float *)malloc(m * k * sizeof(float));
  float *b = (float *)malloc(k * n * sizeof(float));
  float *c = (float *)malloc(m * n * sizeof(float));

  // Initialize a, b, and c

  #pragma acc data copyin(a[0:m*k], b[0:k*n], alpha, beta) copy(c[0:m*n])
  gemm_parallel(m, n, k, alpha, a, m, b, k, beta, c, m);
  #pragma acc wait

  // Use the result in c

  free(a);
  free(b);
  free(c);
  return 0;
}
This code allocates memory for the matrices a, b, and c, initializes them, and then calls the gemm_parallel function using OpenACC's data and wait directives to manage the data transfer between host and device.


Output 8:
Here is the equivalent C/C++ OpenACC code for the given Fortran OpenMP code:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) is equivalent to float in C/C++. If your compiler uses a different type for real(kind=8), you may need to adjust the code accordingly.
Also, OpenACC uses a different syntax and programming model than OpenMP, so the code has been rewritten to take advantage of OpenACC's features. The #pragma acc routine seq directive tells the compiler to generate a sequential version of the routine, while the #pragma acc parallel loop directive parallelizes the loop across the accelerator device. The gang vector clause specifies the parallelization strategy, and the private clause ensures that each thread has its own copy of the variables i, j, l, and temp. The reduction clause is used to perform the reduction operation on the temp variable.
Keep in mind that this is just one possible way to implement the GEMM operation in C/C++ using OpenACC, and there may be other approaches that are more optimal depending on your specific use case.


Output 9:
Here is the equivalent C/C++ OpenACC code for the given Fortran OpenMP code:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) is equivalent to double in C/C++. Also, the lda, ldb, and ldc parameters are used to access the 2D arrays a, b, and c in the C/C++ code, as opposed to the Fortran code where they are used to declare the array dimensions.
To compile this code, you'll need to use a compiler that supports OpenACC, such as GCC or PGI. You may also need to link against an accelerator library, depending on your system configuration.
You can call this function from your C/C++ code like any other function, passing in the appropriate arguments. The #pragma acc directives will instruct the compiler to generate parallel code for the accelerator, if possible.


Output 10:
Here is a version of the code in C/C++ using OpenACC:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  #pragma acc kernels loop independent private(temp)
  for (int j = 0; j < n; j++) {
    for (int i = 0; i < m; i++) {
      double temp = 0.0;
      for (int l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C/C++, arrays are passed to functions as pointers, so the lda, ldb, and ldc parameters are used to compute the correct indexing. Also, the private(temp) clause in the OpenACC directive ensures that each thread has its own private copy of the temp variable.
To compile this code, you would need to use a compiler that supports OpenACC, such as GCC or PGI, and link against an accelerator library such as NVIDIA's cuBLAS.
You can also use OpenACC data directives to move the data to the device and back, like this:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  #pragma acc data copy(a[0:lda*k], b[0:ldb*n], c[0:ldc*n])
  #pragma acc kernels loop independent private(temp)
  for (int j = 0; j < n; j++) {
    for (int i = 0; i < m; i++) {
      double temp = 0.0;
      for (int l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
This way, the data is moved to the device before the computation, and back to the host after the computation.


