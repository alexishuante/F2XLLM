Code Block 1:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int num_blocks_x = (m + BLOCK_SIZE - 1) / BLOCK_SIZE;
  int num_blocks_y = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;

  double *a_d, *b_d, *c_d;

  cudaMalloc((void **)&a_d, lda * k * sizeof(double));
  cudaMalloc((void **)&b_d, ldb * n * sizeof(double));
  cudaMalloc((void **)&c_d, ldc * n * sizeof(double));

  cudaMemcpy(a_d, a, lda * k * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(b_d, b, ldb * n * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(c_d, c, ldc * n * sizeof(double), cudaMemcpyHostToDevice);

  dim3 blocks(num_blocks_x, num_blocks_y);
  dim3 threads(BLOCK_SIZE, BLOCK_SIZE);

  gemm_kernel<<<blocks, threads>>>(m, n, k, alpha, a_d, lda, b_d, ldb, beta, c_d, ldc);

  cudaMemcpy(c, c_d, ldc * n * sizeof(double), cudaMemcpyDeviceToHost);

  cudaFree(a_d);
  cudaFree(b_d);
  cudaFree(c_d);
}

__global__ void gemm_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int tx = threadIdx.x;
  int ty = threadIdx.y;
  int bx = blockIdx.x;
  int by = blockIdx.y;

  int i = ty + by * BLOCK_SIZE;
  int j = tx + bx * BLOCK_SIZE;

  __shared__ double a_shared[BLOCK_SIZE][BLOCK_SIZE];
  __shared__ double b_shared[BLOCK_SIZE][BLOCK_SIZE];

  double temp = 0.0;

  for (int l = 0; l < k; l += BLOCK_SIZE) {
    if (i < m && l + tx < k) {
      a_shared[ty][tx] = a[i * lda + l + tx];
    } else {
      a_shared[ty][tx] = 0.0;
    }
    if (l + ty < k && j < n) {
      b_shared[ty][tx] = b[(l + ty) * ldb + j];
    } else {
      b_shared[ty][tx] = 0.0;
    }
    __syncthreads();

    for (int k = 0; k < BLOCK_SIZE; k++) {
      temp += a_shared[ty][k] * b_shared[k][tx];
    }
    __syncthreads();
  }

  if (i < m && j < n) {
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

Code Block 2:
void gemm_cuda(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  int idy = blockIdx.y * blockDim.y + threadIdx.y;
  if (idx < m && idy < n) {
    temp = 0.0;
    for (l = 0; l < k; l++) {
      temp += a[idx * lda + l] * b[l * ldb + idy];
    }
    c[idx * ldc + idy] = alpha * temp + beta * c[idx * ldc + idy];
  }
}

__global__ void gemm_kernel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  gemm_cuda(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
}

Code Block 3:
void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int num_blocks_x = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;
  int num_blocks_y = (m + BLOCK_SIZE - 1) / BLOCK_SIZE;

  float *a_d, *b_d, *c_d;

  cudaMalloc((void **)&a_d, lda * k * sizeof(float));
  cudaMalloc((void **)&b_d, ldb * n * sizeof(float));
  cudaMalloc((void **)&c_d, ldc * n * sizeof(float));

  cudaMemcpy(a_d, a, lda * k * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(b_d, b, ldb * n * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(c_d, c, ldc * n * sizeof(float), cudaMemcpyHostToDevice);

  dim3 blocks(num_blocks_x, num_blocks_y);
  dim3 threads(BLOCK_SIZE, BLOCK_SIZE);

  gemm_kernel<<<blocks, threads>>>(m, n, k, alpha, a_d, lda, b_d, ldb, beta, c_d, ldc);

  cudaMemcpy(c, c_d, ldc * n * sizeof(float), cudaMemcpyDeviceToHost);

  cudaFree(a_d);
  cudaFree(b_d);
  cudaFree(c_d);
}

__global__ void gemm_kernel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int tx = threadIdx.x;
  int ty = threadIdx.y;
  int bx = blockIdx.x;
  int by = blockIdx.y;

  int i = ty + by * BLOCK_SIZE;
  int j = tx + bx * BLOCK_SIZE;

  __shared__ float a_shared[BLOCK_SIZE][BLOCK_SIZE];
  __shared__ float b_shared[BLOCK_SIZE][BLOCK_SIZE];

  float temp = 0.0;

  for (int l = 0; l < k; l += BLOCK_SIZE) {
    a_shared[ty][tx] = a[i * lda + l + tx];
    b_shared[ty][tx] = b[l * ldb + j];
    __syncthreads();

    for (int n = 0; n < BLOCK_SIZE; n++) {
      temp += a_shared[ty][n] * b_shared[n][tx];
    }
    __syncthreads();
  }

  c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
}

Code Block 4:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  cudaError_t err;

  // Allocate memory on the GPU
  double *a_d, *b_d, *c_d;
  err = cudaMalloc((void **)&a_d, lda * k * sizeof(double));
  err = cudaMalloc((void **)&b_d, ldb * n * sizeof(double));
  err = cudaMalloc((void **)&c_d, ldc * n * sizeof(double));

  // Copy data from host to device
  err = cudaMemcpy(a_d, a, lda * k * sizeof(double), cudaMemcpyHostToDevice);
  err = cudaMemcpy(b_d, b, ldb * n * sizeof(double), cudaMemcpyHostToDevice);
  err = cudaMemcpy(c_d, c, ldc * n * sizeof(double), cudaMemcpyHostToDevice);

  // Launch kernel
  dim3 block(16, 16);
  dim3 grid((m + block.x - 1) / block.x, (n + block.y - 1) / block.y);
  gemm_kernel<<<grid, block>>>(m, n, k, alpha, a_d, lda, b_d, ldb, beta, c_d, ldc);

  // Copy result from device to host
  err = cudaMemcpy(c, c_d, ldc * n * sizeof(double), cudaMemcpyDeviceToHost);

  // Free memory on the GPU
  err = cudaFree(a_d);
  err = cudaFree(b_d);
  err = cudaFree(c_d);
}

__global__ void gemm_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;

  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

Code Block 5:
void gemm_parallel(const int m, const int n, const int k, const double alpha, const double *a, const int lda, const double *b, const int ldb, const double beta, double *c, const int ldc) {
  const int blockSize = 16;
  const int numBlocks = (m + blockSize - 1) / blockSize;

  gemm_parallel_kernel<<<numBlocks, blockSize>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);

  cudaDeviceSynchronize();
}

__global__ void gemm_parallel_kernel(const int m, const int n, const int k, const double alpha, const double *a, const int lda, const double *b, const int ldb, const double beta, double *c, const int ldc) {
  const int row = blockIdx.x * blockDim.x + threadIdx.x;
  const int col = blockIdx.y * blockDim.y + threadIdx.y;

  if (row < m && col < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[row * lda + l] * b[l * ldb + col];
    }
    c[row * ldc + col] = alpha * temp + beta * c[row * ldc + col];
  }
}

Code Block 6:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  int idy = blockIdx.y * blockDim.y + threadIdx.y;
  if (idx < m && idy < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[idx * lda + l] * b[l * ldb + idy];
    }
    c[idx * ldc + idy] = alpha * temp + beta * c[idx * ldc + idy];
  }
}

Code Block 7:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  cudaError_t err;

  // Kernel function
  __global__ void gemm_kernel(double *a, double *b, double *c, int m, int n, int k, double alpha, double beta) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int idy = blockIdx.y * blockDim.y + threadIdx.y;

    if (idx < m && idy < n) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[idx * lda + l] * b[l * ldb + idy];
      }
      c[idx * ldc + idy] = alpha * temp + beta * c[idx * ldc + idy];
    }
  }

  // Launch kernel
  int blockSize = 16;
  int numBlocks = (m * n + blockSize - 1) / blockSize;
  dim3 dimGrid(numBlocks, numBlocks);
  dim3 dimBlock(blockSize, blockSize);
  gemm_kernel<<<dimGrid, dimBlock>>>(a, b, c, m, n, k, alpha, beta);

  // Check for errors
  err = cudaGetLastError();
  if (err != cudaSuccess) {
    printf("Error: %s\n", cudaGetErrorString(err));
  }
}

__global__ void gemm_kernel(double *a, double *b, double *c, int m, int n, int k, double alpha, double beta) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int idy = blockIdx.y * blockDim.y + threadIdx.y;

    if (idx < m && idy < n) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[idx * lda + l] * b[l * ldb + idy];
      }
      c[idx * ldc + idy] = alpha * temp + beta * c[idx * ldc + idy];
    }
  }

Code Block 8:
void gemm_cuda(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int blockSize = 16;
  int numBlocks = (m + blockSize - 1) / blockSize;
  
  gemm_kernel<<<numBlocks, blockSize>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
}

__global__ void gemm_kernel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int row = blockIdx.x * blockDim.x + threadIdx.x;
  int col = blockIdx.y * blockDim.y + threadIdx.y;
  
  if (row < m && col < n) {
    float temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[row * lda + l] * b[l * ldb + col];
    }
    c[row * ldc + col] = alpha * temp + beta * c[row * ldc + col];
  }
}

Code Block 9:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
    int i, j, l;
    double temp;
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    for (i = idx; i < m; i += stride) {
        for (j = 0; j < n; j++) {
            temp = 0.0;
            for (l = 0; l < k; l++) {
                temp += a[i * lda + l] * b[l * ldb + j];
            }
            c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
        }
    }
}

__global__ void gemm_parallel_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
    gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
}

Code Block 10:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  int blockSize = 16;
  int numBlocks = (m + blockSize - 1) / blockSize;
  
  gemm_parallel_kernel<<<numBlocks, blockSize>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
  
  cudaDeviceSynchronize();
}

__global__ void gemm_parallel_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

