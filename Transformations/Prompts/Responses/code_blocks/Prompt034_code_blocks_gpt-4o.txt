Code Block 1:
void gemv_parallel(int n, const float *A, const float *x, float *y) {
    float *d_A, *d_x, *d_y;
    size_t size = n * n * sizeof(float);
    size_t vector_size = n * sizeof(float);

    // Allocate memory on the device
    cudaMalloc((void**)&d_A, size);
    cudaMalloc((void**)&d_x, vector_size);
    cudaMalloc((void**)&d_y, vector_size);

    // Copy memory from host to device
    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, vector_size, cudaMemcpyHostToDevice);

    // Launch the kernel
    int threadsPerBlock = 256;
    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;
    gemv_parallel_kernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_x, d_y, n);

    // Copy result from device to host
    cudaMemcpy(y, d_y, vector_size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void gemv_parallel_kernel(const float *A, const float *x, float *y, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0f;
        for (int j = 0; j < n; j++) {
            sum += A[i * n + j] * x[j];
        }
        y[i] = sum;
    }
}

Code Block 2:
void gemv_parallel(int n, const float* A, const float* x, float* y) {
    float *d_A, *d_x, *d_y;
    
    // Allocate memory on the device
    cudaMalloc((void**)&d_A, n * n * sizeof(float));
    cudaMalloc((void**)&d_x, n * sizeof(float));
    cudaMalloc((void**)&d_y, n * sizeof(float));
    
    // Copy the input data to the device
    cudaMemcpy(d_A, A, n * n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);
    
    // Set the block and grid size
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;
    
    // Launch the kernel
    gemv_kernel<<<gridSize, blockSize>>>(n, d_A, d_x, d_y);
    
    // Copy the result back to the host
    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);
    
    // Free up the device memory
    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void gemv_kernel(int n, const float* A, const float* x, float* y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0f;
        for (int j = 0; j < n; ++j) {
            sum += A[i * n + j] * x[j];
        }
        y[i] = sum;
    }
}

Code Block 3:
void gemv_parallel_cuda(int n, const float *A, const float *x, float *y) {
    float *d_A, *d_x, *d_y;

    cudaMalloc((void **)&d_A, n * n * sizeof(float));
    cudaMalloc((void **)&d_x, n * sizeof(float));
    cudaMalloc((void **)&d_y, n * sizeof(float));

    cudaMemcpy(d_A, A, n * n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);

    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;

    gemv_kernel<<<numBlocks, blockSize>>>(n, d_A, d_x, d_y);

    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);

    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void gemv_kernel(int n, const float *A, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0f;
        for (int j = 0; j < n; j++) {
            sum += A[i * n + j] * x[j];
        }
        y[i] = sum;
    }
}

Code Block 4:
extern "C" void gemv_parallel(int n, const float* A, const float* x, float* y) {
    float *d_A, *d_x, *d_y;

    // Allocate device memory
    cudaMalloc((void**)&d_A, n * n * sizeof(float));
    cudaMalloc((void**)&d_x, n * sizeof(float));
    cudaMalloc((void**)&d_y, n * sizeof(float));

    // Copy data to device
    cudaMemcpy(d_A, A, n * n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);

    // Define block and grid sizes
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;

    // Launch kernel
    gemv_kernel<<<gridSize, blockSize>>>(n, d_A, d_x, d_y);

    // Copy result back to host
    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void gemv_kernel(int n, const float *A, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i < n) {
        float sum = 0.0f;
        for (int j = 0; j < n; ++j) {
            sum += A[i * n + j] * x[j];
        }
        y[i] = sum;
    }
}

Code Block 5:
void gemv_launcher(int n, float *A, float *x, float *y) {
    float *d_A, *d_x, *d_y;

    cudaMalloc((void**)&d_A, n * n * sizeof(float));
    cudaMalloc((void**)&d_x, n * sizeof(float));
    cudaMalloc((void**)&d_y, n * sizeof(float));

    cudaMemcpy(d_A, A, n * n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);

    // Thread block size
    int blockSize = 256;
    // Grid size (number of thread blocks)
    int numBlocks = (n + blockSize - 1) / blockSize;

    // Launch the CUDA kernel
    gemv_parallel<<<numBlocks, blockSize>>>(n, d_A, d_x, d_y);

    // Copy the result back to the host
    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void gemv_parallel(int n, float *A, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0f;
        for (int j = 0; j < n; ++j) {
            sum += A[i * n + j] * x[j];
        }
        y[i] = sum;
    }
}

Code Block 6:
void gemv_parallel(int n, const float *A, const float *x, float *y) {
    float *d_A, *d_x, *d_y;
    size_t size_matrix = n * n * sizeof(float);
    size_t size_vector = n * sizeof(float);

    // Allocate device memory
    cudaMalloc(&d_A, size_matrix);
    cudaMalloc(&d_x, size_vector);
    cudaMalloc(&d_y, size_vector);

    // Copy inputs to device
    cudaMemcpy(d_A, A, size_matrix, cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, size_vector, cudaMemcpyHostToDevice);

    // Determine block and grid sizes
    int blockSize = 256;  // Adjust as necessary
    int numBlocks = (n + blockSize - 1) / blockSize;

    // Launch kernel
    gemvKernel<<<numBlocks, blockSize>>>(n, d_A, d_x, d_y);

    // Copy result back to host
    cudaMemcpy(y, d_y, size_vector, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void gemvKernel(int n, const float *A, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0f;
        for (int j = 0; j < n; ++j) {
            sum += A[i * n + j] * x[j];
        }
        y[i] = sum;
    }
}

Code Block 7:
void gemv_parallel(int n, float *A, float *x, float *y) {
    float *d_A, *d_x, *d_y;
    size_t size = n * n * sizeof(float);
    size_t vec_size = n * sizeof(float);

    // Allocate device memory
    cudaMalloc((void **)&d_A, size);
    cudaMalloc((void **)&d_x, vec_size);
    cudaMalloc((void **)&d_y, vec_size);

    // Copy data from host to device memory
    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, vec_size, cudaMemcpyHostToDevice);

    // Determine grid and block sizes
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;

    // Launch the kernel
    gemv_parallel_kernel<<<gridSize, blockSize>>>(n, d_A, d_x, d_y);

    // Copy result from device to host
    cudaMemcpy(y, d_y, vec_size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void gemv_parallel_kernel(int n, float *A, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i < n) {
        float sum = 0.0f;
        for (int j = 0; j < n; j++) {
            sum += A[i * n + j] * x[j];
        }
        y[i] = sum;
    }
}

Code Block 8:
void call_gemv_parallel(int n, float *A, float *x, float *y) {
    float *d_A, *d_x, *d_y;
  
    // Allocate device memory
    cudaMalloc((void**) &d_A, n*n*sizeof(float));
    cudaMalloc((void**) &d_x, n*sizeof(float));
    cudaMalloc((void**) &d_y, n*sizeof(float));

    // Copy data to device
    cudaMemcpy(d_A, A, n*n*sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n*sizeof(float), cudaMemcpyHostToDevice);

    // Execute kernel
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;
    gemv_parallel<<<gridSize, blockSize>>>(n, d_A, d_x, d_y);
  
    // Copy result back to host
    cudaMemcpy(y, d_y, n*sizeof(float), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void gemv_parallel(int n, float *A, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0;
        for (int j = 0; j < n; ++j) {
            sum += A[i * n + j] * x[j];
        }
        y[i] = sum;
    }
}

Code Block 9:
void gemv_parallel(int n, float *A, float *x, float *y) {
    float *d_A, *d_x, *d_y;

    // Allocate device memory
    cudaMalloc((void **)&d_A, n * n * sizeof(float));
    cudaMalloc((void **)&d_x, n * sizeof(float));
    cudaMalloc((void **)&d_y, n * sizeof(float));

    // Copy data from host to device
    cudaMemcpy(d_A, A, n * n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);

    // Define block and grid dimensions
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;

    // Launch the CUDA kernel
    gemv_kernel<<<gridSize, blockSize>>>(n, d_A, d_x, d_y);

    // Copy result from device to host
    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void gemv_kernel(int n, float *A, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0f;
        for (int j = 0; j < n; j++) {
            sum += A[i * n + j] * x[j];
        }
        y[i] = sum;
    }
}

Code Block 10:
void gemv_parallel(int n, const float *A, const float *x, float *y) {
    float *d_A, *d_x, *d_y;
    size_t size_matrix = n * n * sizeof(float);
    size_t size_vector = n * sizeof(float);

    // Allocate device memory
    cudaMalloc((void **)&d_A, size_matrix);
    cudaMalloc((void **)&d_x, size_vector);
    cudaMalloc((void **)&d_y, size_vector);

    // Copy data from host to device
    cudaMemcpy(d_A, A, size_matrix, cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, size_vector, cudaMemcpyHostToDevice);

    // Define the block and grid sizes
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;

    // Launch the kernel
    gemv_kernel<<<gridSize, blockSize>>>(n, d_A, d_x, d_y);

    // Copy result from device to host
    cudaMemcpy(y, d_y, size_vector, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_x);
    cudaFree(d_y);
}

__global__ void gemv_kernel(int n, const float *A, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i < n) {
        float sum = 0.0f;
        for (int j = 0; j < n; ++j) {
            sum += A[i * n + j] * x[j];
        }
        y[i] = sum;
    }
}

