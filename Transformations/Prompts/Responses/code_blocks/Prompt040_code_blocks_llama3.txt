Code Block 1:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i, j;
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  
  // CUDA kernel function
  __global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
      float sum = 0.0;
      for (j = row[idx]; j < row[idx+1]; j++) {
        sum += val[j] * x[col[j]];
      }
      y[idx] = sum;
    }
  }
  
  // Launch CUDA kernel
  spmv_kernel<<<numBlocks, blockSize>>>(n, nnz, val, row, col, x, y);
  
  // Check for errors
  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    printf("Error: %s\n", cudaGetErrorString(err));
  }
}

__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
      float sum = 0.0;
      for (j = row[idx]; j < row[idx+1]; j++) {
        sum += val[j] * x[col[j]];
      }
      y[idx] = sum;
    }
  }

Code Block 2:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i, j;
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  
  // CUDA kernel
  __global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int idx = blockIdx.x * blockSize + threadIdx.x;
    if (idx < n) {
      float sum = 0.0;
      for (j = row[idx]; j < row[idx+1]; j++) {
        sum += val[j] * x[col[j]];
      }
      y[idx] = sum;
    }
  }
  
  // Launch CUDA kernel
  spmv_kernel<<<numBlocks, blockSize>>>(n, nnz, val, row, col, x, y);
  
  // Synchronize threads
  cudaDeviceSynchronize();
}

__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int idx = blockIdx.x * blockSize + threadIdx.x;
    if (idx < n) {
      float sum = 0.0;
      for (j = row[idx]; j < row[idx+1]; j++) {
        sum += val[j] * x[col[j]];
      }
      y[idx] = sum;
    }
  }

Code Block 3:
void spmv_parallel_cuda(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;

  spmv_kernel<<<numBlocks, blockSize>>>(n, nnz, val, row, col, x, y);

  cudaDeviceSynchronize();
}

__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;

  if (idx < n) {
    float sum = 0.0;
    for (int j = row[idx]; j < row[idx+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[idx] = sum;
  }
}

Code Block 4:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n) {
    float sum = 0.0;
    for (int j = row[idx]; j < row[idx+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[idx] = sum;
  }
}

Code Block 5:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;

  spmv_kernel<<<numBlocks, blockSize>>>(n, nnz, val, row, col, x, y);

  cudaDeviceSynchronize();
}

__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n) {
    float sum = 0.0;
    for (int j = row[idx]; j < row[idx + 1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[idx] = sum;
  }
}

Code Block 6:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n) {
    float sum = 0.0;
    for (int j = row[idx]; j < row[idx+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[idx] = sum;
  }
}

Code Block 7:
void spmv(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;
    
    cudaMalloc((void **)&d_val, nnz * sizeof(float));
    cudaMalloc((void **)&d_x, n * sizeof(float));
    cudaMalloc((void **)&d_y, n * sizeof(float));
    cudaMalloc((void **)&d_row, (n+1) * sizeof(int));
    cudaMalloc((void **)&d_col, nnz * sizeof(int));
    
    cudaMemcpy(d_val, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_row, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_col, col, nnz * sizeof(int), cudaMemcpyHostToDevice);
    
    spmv_parallel<<<numBlocks, blockSize>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);
    
    cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);
    
    cudaFree(d_val);
    cudaFree(d_x);
    cudaFree(d_y);
    cudaFree(d_row);
    cudaFree(d_col);
}

__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float sum = 0.0;
        for (int j = row[idx]; j < row[idx+1]-1; j++) {
            sum += val[j] * x[col[j]];
        }
        y[idx] = sum;
    }
}

Code Block 8:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  spmv_kernel<<<numBlocks, blockSize>>>(n, nnz, val, row, col, x, y);
  cudaDeviceSynchronize();
}

__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n) {
    float sum = 0.0;
    for (int j = row[idx]; j < row[idx+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[idx] = sum;
  }
}

Code Block 9:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;

  spmv_kernel<<<numBlocks, blockSize>>>(n, nnz, val, row, col, x, y);

  cudaDeviceSynchronize();
}

__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;

  if (idx < n) {
    float sum = 0.0;
    for (int j = row[idx]; j < row[idx+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[idx] = sum;
  }
}

Code Block 10:
void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n) {
    float sum = 0.0;
    for (int j = row[idx]; j < row[idx+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[idx] = sum;
  }
}

