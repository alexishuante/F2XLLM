Output 1:
Here are the equivalent functions in C, HIP, Fortran, and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
HIP:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$OMP PARALLEL DO COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$OMP END PARALLEL DO
end subroutine gemm_parallel
OpenACC:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$ACC END PARALLEL LOOP
end subroutine gemm_parallel
Note that the HIP version is a CUDA kernel, and the Fortran and OpenACC versions use OpenMP and OpenACC directives, respectively, to parallelize the loop. The C version uses OpenMP.


Output 2:
Here are the equivalent functions in C, HIP, Fortran, and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
HIP:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran:
SUBROUTINE gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  IMPLICIT NONE
  INTEGER, INTENT(IN) :: m, n, k, lda, ldb, ldc
  REAL(KIND=8), INTENT(IN) :: alpha, beta
  REAL(KIND=8), INTENT(IN) :: a(lda, k), b(ldb, n)
  REAL(KIND=8), INTENT(INOUT) :: c(ldc, n)
  INTEGER :: i, j, l
  REAL(KIND=8) :: temp
!$OMP PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  DO j = 1, n
    DO i = 1, m
      temp = 0.0
      DO l = 1, k
        temp = temp + a(i, l) * b(l, j)
      END DO
      c(i, j) = alpha * temp + beta * c(i, j)
    END DO
  END DO
!$OMP END PARALLEL LOOP
END SUBROUTINE gemm_parallel
OpenACC:
SUBROUTINE gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  IMPLICIT NONE
  INTEGER, INTENT(IN) :: m, n, k, lda, ldb, ldc
  REAL(KIND=8), INTENT(IN) :: alpha, beta
  REAL(KIND=8), INTENT(IN) :: a(lda, k), b(ldb, n)
  REAL(KIND=8), INTENT(INOUT) :: c(ldc, n)
  INTEGER :: i, j, l
  REAL(KIND=8) :: temp
!$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  DO j = 1, n
    DO i = 1, m
      temp = 0.0
      DO l = 1, k
        temp = temp + a(i, l) * b(l, j)
      END DO
      c(i, j) = alpha * temp + beta * c(i, j)
    END DO
  END DO
!$ACC END PARALLEL LOOP
END SUBROUTINE gemm_parallel
Note that the HIP version is a GPU kernel, and the Fortran and OpenACC versions are similar, with the main difference being the directives used for parallelization (!$OMP for OpenMP and !$ACC for OpenACC).


Output 3:
Here are the equivalent functions in C, HIP, Fortran, and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
HIP:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran with OpenACC:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$ACC END PARALLEL LOOP
end subroutine gemm_parallel
Note that the HIP version is a GPU kernel, and the number of blocks and threads per block need to be configured when launching the kernel. Also, the Fortran version uses OpenACC directives to parallelize the loop, while the C version uses OpenMP.


Output 4:
Here are the implementations of the GEMM subroutine in C and HIP, and Fortran and OpenACC:
C Implementation:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
HIP Implementation:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran Implementation with OpenACC:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$ACC END PARALLEL LOOP
end subroutine gemm_parallel
Note that the HIP implementation is a CUDA kernel, and it is assumed that the data has been transferred to the GPU and the kernel is launched with appropriate block and grid dimensions. Also, the Fortran implementation with OpenACC is similar to the original code, but with the addition of OpenACC directives to parallelize the loops.


Output 5:
Here are the equivalent functions in C, HIP, Fortran, and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
HIP:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$OMP PARALLEL DO COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$OMP END PARALLEL DO
end subroutine gemm_parallel
OpenACC:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$ACC END PARALLEL LOOP
end subroutine gemm_parallel
Note that the HIP version is a GPU kernel, and the Fortran and OpenACC versions use OpenMP and OpenACC directives, respectively, to parallelize the loop.


Output 6:
Here are the equivalent functions in C and HIP, and Fortran and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
HIP:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran with OpenACC:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$ACC END PARALLEL LOOP
end subroutine gemm_parallel
Note that the HIP version is a CUDA kernel, and you would need to launch it using the hipLaunchKernel function. Also, the Fortran version is identical to the one you provided, as OpenACC is a directive-based parallelization model that can be used with Fortran.


Output 7:
Here are the equivalent functions in C, HIP, Fortran, and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
HIP:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran:
SUBROUTINE gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  IMPLICIT NONE
  INTEGER, INTENT(IN) :: m, n, k, lda, ldb, ldc
  REAL(KIND=8), INTENT(IN) :: alpha, beta
  REAL(KIND=8), INTENT(IN) :: a(lda, k), b(ldb, n)
  REAL(KIND=8), INTENT(INOUT) :: c(ldc, n)
  INTEGER :: i, j, l
  REAL(KIND=8) :: temp
!$OMP PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  DO j = 1, n
    DO i = 1, m
      temp = 0.0
      DO l = 1, k
        temp = temp + a(i, l) * b(l, j)
      END DO
      c(i, j) = alpha * temp + beta * c(i, j)
    END DO
  END DO
!$OMP END PARALLEL LOOP
END SUBROUTINE gemm_parallel
OpenACC:
SUBROUTINE gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  IMPLICIT NONE
  INTEGER, INTENT(IN) :: m, n, k, lda, ldb, ldc
  REAL(KIND=8), INTENT(IN) :: alpha, beta
  REAL(KIND=8), INTENT(IN) :: a(lda, k), b(ldb, n)
  REAL(KIND=8), INTENT(INOUT) :: c(ldc, n)
  INTEGER :: i, j, l
  REAL(KIND=8) :: temp
!$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  DO j = 1, n
    DO i = 1, m
      temp = 0.0
      DO l = 1, k
        temp = temp + a(i, l) * b(l, j)
      END DO
      c(i, j) = alpha * temp + beta * c(i, j)
    END DO
  END DO
!$ACC END PARALLEL LOOP
END SUBROUTINE gemm_parallel
Note that the HIP version is a CUDA kernel, and the Fortran and OpenACC versions are identical, with the only difference being the directive used (!$OMP for OpenMP or !$ACC for OpenACC).


Output 8:
Here are the implementations of the GEMM operation in C, HIP, Fortran, and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
HIP:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran with OpenACC:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$ACC END PARALLEL LOOP
end subroutine gemm_parallel
Note that the HIP implementation is a CUDA kernel, and the Fortran with OpenACC implementation is similar to the one you provided. The C implementation uses OpenMP for parallelization.


Output 9:
Here are the implementations of the GEMM operation in C, HIP, Fortran, and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
HIP:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$OMP PARALLEL DO COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$OMP END PARALLEL DO
end subroutine gemm_parallel
OpenACC:
subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  implicit none
  integer, intent(in) :: m, n, k, lda, ldb, ldc
  real(kind=8), intent(in) :: alpha, beta
  real(kind=8), intent(in) :: a(lda, k), b(ldb, n)
  real(kind=8), intent(inout) :: c(ldc, n)
  integer :: i, j, l
  real(kind=8) :: temp
  !$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  do j = 1, n
    do i = 1, m
      temp = 0.0
      do l = 1, k
        temp = temp + a(i, l) * b(l, j)
      end do
      c(i, j) = alpha * temp + beta * c(i, j)
    end do
  end do
  !$ACC END PARALLEL LOOP
end subroutine gemm_parallel
Note that the HIP implementation is a CUDA kernel, and the Fortran and OpenACC implementations use OpenMP and OpenACC directives, respectively, to parallelize the loop. The C implementation uses OpenMP directives.


Output 10:
Here are the implementations of the GEMM operation in C, HIP, Fortran, and OpenACC:
C:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
HIP:
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}
Fortran:
SUBROUTINE gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  IMPLICIT NONE
  INTEGER, INTENT(IN) :: m, n, k, lda, ldb, ldc
  REAL(KIND=8), INTENT(IN) :: alpha, beta
  REAL(KIND=8), INTENT(IN) :: a(lda, k), b(ldb, n)
  REAL(KIND=8), INTENT(INOUT) :: c(ldc, n)
  INTEGER :: i, j, l
  REAL(KIND=8) :: temp
!$OMP PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  DO j = 1, n
    DO i = 1, m
      temp = 0.0
      DO l = 1, k
        temp = temp + a(i, l) * b(l, j)
      END DO
      c(i, j) = alpha * temp + beta * c(i, j)
    END DO
  END DO
!$OMP END PARALLEL LOOP
END SUBROUTINE gemm_parallel
OpenACC:
SUBROUTINE gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
  IMPLICIT NONE
  INTEGER, INTENT(IN) :: m, n, k, lda, ldb, ldc
  REAL(KIND=8), INTENT(IN) :: alpha, beta
  REAL(KIND=8), INTENT(IN) :: a(lda, k), b(ldb, n)
  REAL(KIND=8), INTENT(INOUT) :: c(ldc, n)
  INTEGER :: i, j, l
  REAL(KIND=8) :: temp
!$ACC PARALLEL LOOP COLLAPSE(2) PRIVATE(i, j, l, temp)
  DO j = 1, n
    DO i = 1, m
      temp = 0.0
      DO l = 1, k
        temp = temp + a(i, l) * b(l, j)
      END DO
      c(i, j) = alpha * temp + beta * c(i, j)
    END DO
  END DO
!$ACC END PARALLEL LOOP
END SUBROUTINE gemm_parallel
Note that the HIP implementation is a kernel that needs to be launched on the GPU, and the number of blocks and threads per block needs to be specified. The Fortran and OpenACC implementations use directives to parallelize the loop.


