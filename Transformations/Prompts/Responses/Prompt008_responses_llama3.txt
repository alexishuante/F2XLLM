Output 1:
Here is the equivalent C/C++ OpenMP code for the given Fortran OpenACC code:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
  int i, j, k, iter;
#pragma omp parallel private(i, j, k)
  for (iter = 1; iter <= niter; iter++) {
#pragma omp for collapse(3)
    for (k = 2; k < n - 1; k++) {
      for (j = 2; j < n - 1; j++) {
        for (i = 2; i < n - 1; i++) {
          unew[i][j][k] = 0.125 * (u[i - 1][j][k] + u[i + 1][j][k] + u[i][j - 1][k] +
            u[i][j + 1][k] + u[i][j][k - 1] + u[i][j][k + 1] + u[i][j][k]);
        }
      }
    }
#pragma omp barrier
#pragma omp single
    for (k = 0; k < n; k++) {
      for (j = 0; j < n; j++) {
        for (i = 0; i < n; i++) {
          u[i][j][k] = unew[i][j][k];
        }
      }
    }
  }
}
Note that in C/C++, we need to use pointers (or references) for the 3D arrays, and we also need to explicitly allocate and deallocate memory for the arrays. Also, the collapse(3) clause is not directly supported in OpenMP, so we use three nested for loops instead. Finally, we use #pragma omp barrier to ensure that all threads have finished the computation before copying the data from unew back to u.


Output 2:
Here is the equivalent C/C++ OpenMP code for the given Fortran OpenACC code:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
    int i, j, k, iter;
    #pragma omp parallel private(i, j, k)
    {
        for (iter = 0; iter < niter; iter++) {
            #pragma omp for collapse(3)
            for (k = 1; k < n - 1; k++) {
                for (j = 1; j < n - 1; j++) {
                    for (i = 1; i < n - 1; i++) {
                        unew[k][j][i] = 0.125 * (u[k][j][i-1] + u[k][j][i+1] + u[k][j-1][i] +
                                                     u[k][j+1][i] + u[k-1][j][i] + u[k+1][j][i] + u[k][j][i]);
                    }
                }
            }
            #pragma omp barrier
            #pragma omp for collapse(3)
            for (k = 1; k < n - 1; k++) {
                for (j = 1; j < n - 1; j++) {
                    for (i = 1; i < n - 1; i++) {
                        u[k][j][i] = unew[k][j][i];
                    }
                }
            }
        }
    }
}
Note that in C/C++, array indices start at 0, so the loop bounds are adjusted accordingly. Also, the collapse(3) clause is used to collapse the three nested loops into a single loop, and the barrier directive is used to ensure that all threads have finished the first loop before starting the second loop.


Output 3:
Here is the equivalent C/C++ OpenMP code for the given Fortran OpenACC code:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
  int i, j, k, iter;
  #pragma omp parallel for collapse(3) private(i, j, k)
  for (iter = 0; iter < niter; iter++) {
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[i][j][k] = 0.125 * (u[i-1][j][k] + u[i+1][j][k] + u[i][j-1][k] +
            u[i][j+1][k] + u[i][j][k-1] + u[i][j][k+1] + u[i][j][k]);
        }
      }
    }
  }
  #pragma omp parallel for collapse(3)
  for (k = 0; k < n; k++) {
    for (j = 0; j < n; j++) {
      for (i = 0; i < n; i++) {
        u[i][j][k] = unew[i][j][k];
      }
    }
  }
}
Note that in C/C++, arrays are 0-indexed, so the loop bounds are adjusted accordingly. Also, the collapse(3) clause is used to collapse the three nested loops into a single parallel loop. The private(i, j, k) clause is used to make the loop indices private to each thread.


Output 4:
Here is the equivalent C/C++ OpenMP code for the given Fortran OpenACC code:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
  int i, j, k, iter;
  #pragma omp parallel private(i, j, k)
  {
    for (iter = 0; iter < niter; iter++) {
      #pragma omp for collapse(3)
      for (k = 1; k < n - 1; k++) {
        for (j = 1; j < n - 1; j++) {
          for (i = 1; i < n - 1; i++) {
            unew[i][j][k] = 0.125 * (u[i-1][j][k] + u[i+1][j][k] + u[i][j-1][k] +
              u[i][j+1][k] + u[i][j][k-1] + u[i][j][k+1] + u[i][j][k]);
          }
        }
      }
      #pragma omp barrier
      #pragma omp single
      {
        for (k = 0; k < n; k++) {
          for (j = 0; j < n; j++) {
            for (i = 0; i < n; i++) {
              u[i][j][k] = unew[i][j][k];
            }
          }
        }
      }
    }
  }
}
Note that in C/C++, arrays are 0-based, so the loop indices start from 0 instead of 1. Also, the collapse(3) clause is used to collapse the three nested loops into a single parallel loop. The barrier directive is used to ensure that all threads have finished the computation before the data is copied back to u. The single directive is used to serialize the data copy operation.


Output 5:
Here is the equivalent C/C++ OpenMP code for the given Fortran OpenACC code:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
  int i, j, k, iter;
  #pragma omp parallel for collapse(3) private(i, j, k)
  for (iter = 0; iter < niter; iter++) {
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[i][j][k] = 0.125 * (u[i-1][j][k] + u[i+1][j][k] + u[i][j-1][k] +
            u[i][j+1][k] + u[i][j][k-1] + u[i][j][k+1] + u[i][j][k]);
        }
      }
    }
  }
  #pragma omp parallel for collapse(3)
  for (k = 0; k < n; k++) {
    for (j = 0; j < n; j++) {
      for (i = 0; i < n; i++) {
        u[i][j][k] = unew[i][j][k];
      }
    }
  }
}
Note that in C/C++, arrays are 0-based, so the loop bounds are adjusted accordingly. Also, the collapse(3) clause is used to collapse the three nested loops into a single parallel loop, similar to the COLLAPSE(3) clause in OpenACC. The private(i, j, k) clause is used to make the loop indices private to each thread, as in the OpenACC code.


Output 6:
Here is the equivalent C/C++ OpenMP code for the given Fortran OpenACC code:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
  int i, j, k, iter;
  #pragma omp parallel private(i, j, k)
  {
    for (iter = 0; iter < niter; iter++) {
      #pragma omp for collapse(3)
      for (k = 1; k < n - 1; k++) {
        for (j = 1; j < n - 1; j++) {
          for (i = 1; i < n - 1; i++) {
            unew[i][j][k] = 0.125 * (u[i-1][j][k] + u[i+1][j][k] + u[i][j-1][k] +
              u[i][j+1][k] + u[i][j][k-1] + u[i][j][k+1] + u[i][j][k]);
          }
        }
      }
      #pragma omp barrier
      #pragma omp single
      {
        for (k = 0; k < n; k++) {
          for (j = 0; j < n; j++) {
            for (i = 0; i < n; i++) {
              u[i][j][k] = unew[i][j][k];
            }
          }
        }
      }
    }
  }
}
Note that in C/C++, arrays are 0-based, so the loop indices start from 0 instead of 1. Also, the collapse(3) clause is used to collapse the three nested loops into a single loop, and the barrier directive is used to ensure that all threads have finished the computation before copying the data back to u.


Output 7:
Here is the equivalent C/C++ OpenMP code for the given Fortran OpenACC code:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
    int i, j, k, iter;
    #pragma omp parallel private(i, j, k)
    {
        for (iter = 0; iter < niter; iter++) {
            #pragma omp for collapse(3)
            for (k = 1; k < n - 1; k++) {
                for (j = 1; j < n - 1; j++) {
                    for (i = 1; i < n - 1; i++) {
                        unew[k][j][i] = 0.125 * (u[k][j][i-1] + u[k][j][i+1] + u[k][j-1][i] +
                            u[k][j+1][i] + u[k-1][j][i] + u[k+1][j][i] + u[k][j][i]);
                    }
                }
            }
            #pragma omp barrier
            #pragma omp single
            {
                for (k = 0; k < n; k++) {
                    for (j = 0; j < n; j++) {
                        for (i = 0; i < n; i++) {
                            u[k][j][i] = unew[k][j][i];
                        }
                    }
                }
            }
        }
    }
}
Note that in C/C++, arrays are 0-based, so the loop bounds are adjusted accordingly. Also, the collapse(3) clause is used to collapse the three nested loops into a single loop, similar to the Fortran OpenACC code. The barrier directive is used to ensure that all threads have finished the computation before the data is copied back to u. The single directive is used to serialize the copy operation.


Output 8:
Here is the equivalent C/C++ OpenMP code for the given Fortran OpenACC code:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
    int i, j, k, iter;
    #pragma omp parallel private(i, j, k)
    for (iter = 0; iter < niter; iter++) {
        #pragma omp for collapse(3)
        for (k = 1; k < n - 1; k++) {
            for (j = 1; j < n - 1; j++) {
                for (i = 1; i < n - 1; i++) {
                    unew[i][j][k] = 0.125 * (u[i-1][j][k] + u[i+1][j][k] + u[i][j-1][k] +
                        u[i][j+1][k] + u[i][j][k-1] + u[i][j][k+1] + u[i][j][k]);
                }
            }
        }
        #pragma omp barrier
        #pragma omp for collapse(3)
        for (k = 1; k < n - 1; k++) {
            for (j = 1; j < n - 1; j++) {
                for (i = 1; i < n - 1; i++) {
                    u[i][j][k] = unew[i][j][k];
                }
            }
        }
    }
}
Note that in C/C++, array indices start at 0, so the loop bounds are adjusted accordingly. Also, the collapse(3) clause is used to collapse the three nested loops into a single loop, and the private(i, j, k) clause is used to make the loop indices private to each thread. The barrier directive is used to ensure that all threads have finished the first loop before moving on to the second loop.


Output 9:
Here is the equivalent C/C++ OpenMP code for the given Fortran OpenACC code:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
  int i, j, k, iter;
  #pragma omp parallel private(i, j, k)
  {
    for (iter = 0; iter < niter; iter++) {
      #pragma omp for collapse(3)
      for (k = 1; k < n - 1; k++) {
        for (j = 1; j < n - 1; j++) {
          for (i = 1; i < n - 1; i++) {
            unew[i][j][k] = 0.125 * (u[i - 1][j][k] + u[i + 1][j][k] + u[i][j - 1][k] +
                                       u[i][j + 1][k] + u[i][j][k - 1] + u[i][j][k + 1] + u[i][j][k]);
          }
        }
      }
      #pragma omp barrier
      #pragma omp for collapse(3)
      for (k = 1; k < n - 1; k++) {
        for (j = 1; j < n - 1; j++) {
          for (i = 1; i < n - 1; i++) {
            u[i][j][k] = unew[i][j][k];
          }
        }
      }
    }
  }
}
Note that in C/C++, arrays are 0-based, so the loop indices start from 0, unlike in Fortran where they start from 1. Also, the collapse(3) clause is used to collapse the three nested loops into a single loop, and the private(i, j, k) clause is used to make the loop indices private to each thread. Finally, the barrier directive is used to ensure that all threads have finished the computation before the data is copied back to the original array u.


Output 10:
Here is the equivalent C/C++ OpenMP code for the given Fortran OpenACC code:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
  int i, j, k, iter;
  #pragma omp parallel for collapse(3) private(i, j, k)
  for (iter = 0; iter < niter; iter++) {
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[k][j][i] = 0.125 * (u[k][j][i-1] + u[k][j][i+1] + u[k][j-1][i] +
            u[k][j+1][i] + u[k-1][j][i] + u[k+1][j][i] + u[k][j][i]);
        }
      }
    }
  }
  #pragma omp parallel for collapse(3)
  for (k = 0; k < n; k++) {
    for (j = 0; j < n; j++) {
      for (i = 0; i < n; i++) {
        u[k][j][i] = unew[k][j][i];
      }
    }
  }
}
Note that in C/C++, arrays are 0-based, so the loop indices start from 0 instead of 1. Also, the collapse(3) clause is used to collapse the three nested loops into a single parallel loop. The private(i, j, k) clause is used to make the loop indices private to each thread.
Also, the array indexing is different in C/C++ compared to Fortran. In C/C++, the array indexing is done using u[k][j][i] instead of u(i, j, k).
It's worth noting that the above code assumes that the 3D arrays u and unew are already allocated and initialized. In C/C++, you would need to allocate the memory for the arrays using pointers, like this:
double ***u = new double **[n];
for (int k = 0; k < n; k++) {
  u[k] = new double *[n];
  for (int j = 0; j < n; j++) {
    u[k][j] = new double[n];
  }
}
And similarly for the unew array.


