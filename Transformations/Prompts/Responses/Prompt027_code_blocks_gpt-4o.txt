Code Block 1:
void spmv_parallel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y) {
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;
    
    // Allocate device memory
    hipMalloc((void**)&d_val, nnz * sizeof(float));
    hipMalloc((void**)&d_row, (n+1) * sizeof(int));
    hipMalloc((void**)&d_col, nnz * sizeof(int));
    hipMalloc((void**)&d_x, n * sizeof(float));
    hipMalloc((void**)&d_y, n * sizeof(float));
    
    // Copy data from host to device
    hipMemcpy(d_val, val, nnz * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_row, row, (n+1) * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_col, col, nnz * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    
    // Define the execution configuration
    int blockSize = 256; // Number of threads in each block
    int numBlocks = (n + blockSize - 1) / blockSize;
    
    // Launch the kernel
    hipLaunchKernelGGL(spmv_kernel, dim3(numBlocks), dim3(blockSize), 0, 0, n, nnz, d_val, d_row, d_col, d_x, d_y);
    
    // Copy the result from device to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);
    
    // Free device memory
    hipFree(d_val);
    hipFree(d_row);
    hipFree(d_col);
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void spmv_kernel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < n) {
        float temp = 0.0f;
        for (int j = row[i]; j < row[i+1]; j++) {
            temp += val[j] * x[col[j]];
        }
        y[i] = temp;
    }
}

Code Block 2:
void spmv_hip(int n, int nnz, const float* val, const int* row, const int* col, const float* x, float* y) {
    // Device pointers
    float* d_val;
    int* d_row;
    int* d_col;
    float* d_x;
    float* d_y;

    // Allocate device memory
    hipMalloc((void**)&d_val, nnz * sizeof(float));
    hipMalloc((void**)&d_row, (n + 1) * sizeof(int));
    hipMalloc((void**)&d_col, nnz * sizeof(int));
    hipMalloc((void**)&d_x, n * sizeof(float));
    hipMalloc((void**)&d_y, n * sizeof(float));

    // Copy data to device
    hipMemcpy(d_val, val, nnz * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_row, row, (n + 1) * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_col, col, nnz * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);

    // Launch kernel
    int threadsPerBlock = 256;
    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;
    hipLaunchKernelGGL(spmv_kernel, dim3(blocksPerGrid), dim3(threadsPerBlock), 0, 0, n, nnz, d_val, d_row, d_col, d_x, d_y);

    // Copy result back to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_val);
    hipFree(d_row);
    hipFree(d_col);
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void spmv_kernel(int n, int nnz, const float* val, const int* row, const int* col, const float* x, float* y) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;

    if (i < n) {
        float sum = 0.0f;
        for (int j = row[i]; j < row[i+1]; ++j) {
            sum += val[j] * x[col[j]];
        }
        y[i] = sum;
    }
}

Code Block 3:
extern "C" void spmv_parallel(const int n, const int nnz, 
                              const float *val, const int *row, 
                              const int *col, const float *x, float *y) {
    float *d_val;
    int *d_row, *d_col;
    float *d_x, *d_y;

    // Allocate device memory
    hipMalloc(&d_val, nnz * sizeof(float));
    hipMalloc(&d_row, (n + 1) * sizeof(int));
    hipMalloc(&d_col, nnz * sizeof(int));
    hipMalloc(&d_x, n * sizeof(float));
    hipMalloc(&d_y, n * sizeof(float));

    // Copy data from host to device
    hipMemcpy(d_val, val, nnz * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_row, row, (n + 1) * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_col, col, nnz * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);

    // Determine grid and block dimensions
    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;

    // Launch the kernel on the GPU
    hipLaunchKernelGGL(spmv_kernel, numBlocks, blockSize, 0, 0, n, nnz, d_val, d_row, d_col, d_x, d_y);

    // Copy the result from device to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_val);
    hipFree(d_row);
    hipFree(d_col);
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void spmv_kernel(const int n, const int nnz, 
                            const float *val, const int *row, 
                            const int *col, const float *x, float *y) {
    int i = threadIdx.x + blockIdx.x * blockDim.x;

    if (i < n) {
        float result = 0.0f;
        for (int j = row[i]; j < row[i + 1]; ++j) {
            result += val[j] * x[col[j]];
        }
        y[i] = result;
    }
}

Code Block 4:
void spmv_parallel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y) {
    // Allocate device memory
    float *d_val = NULL, *d_x = NULL, *d_y = NULL;
    int *d_row = NULL, *d_col = NULL;

    hipMalloc((void**)&d_val, nnz * sizeof(float));
    hipMalloc((void**)&d_x, n * sizeof(float));
    hipMalloc((void**)&d_y, n * sizeof(float));
    hipMalloc((void**)&d_row, (n + 1) * sizeof(int));
    hipMalloc((void**)&d_col, nnz * sizeof(int));

    // Copy data to device
    hipMemcpy(d_val, val, nnz * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_row, row, (n + 1) * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_col, col, nnz * sizeof(int), hipMemcpyHostToDevice);

    // Determine launch configuration
    int blockSize = 256;  // Number of threads per block
    int numBlocks = (n + blockSize - 1) / blockSize;  // Number of blocks

    // Launch kernel
    hipLaunchKernelGGL(spmv_kernel, dim3(numBlocks), dim3(blockSize), 0, 0, n, d_row, d_col, d_val, d_x, d_y);

    // Copy result back to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_val);
    hipFree(d_x);
    hipFree(d_y);
    hipFree(d_row);
    hipFree(d_col);
}

__global__ void spmv_kernel(int n, const int *row, const int *col, const float *val, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0f;
        for (int j = row[i]; j < row[i + 1]; j++) {
            sum += val[j] * x[col[j]];
        }
        y[i] = sum;
    }
}

Code Block 5:
void spmv_parallel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y) {
    int i, j;

    #pragma omp parallel for default(none) shared(n, nnz, val, row, col, x, y) private(i, j)
    for (i = 0; i < n; i++) {
        y[i] = 0.0;
        for (j = row[i]; j < row[i + 1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

__global__ void spmv_kernel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i < n) {
        float sum = 0.0f;
        for (int j = row[i]; j < row[i + 1]; j++) {
            sum += val[j] * x[col[j]];
        }
        y[i] = sum;
    }
}

Code Block 6:
void spmv_parallel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y) {
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;

    // Allocate device memory
    hipMalloc((void**)&d_val, nnz * sizeof(float));
    hipMalloc((void**)&d_x, n * sizeof(float));
    hipMalloc((void**)&d_y, n * sizeof(float));
    hipMalloc((void**)&d_row, (n + 1) * sizeof(int));
    hipMalloc((void**)&d_col, nnz * sizeof(int));

    // Copy data to device
    hipMemcpy(d_val, val, nnz * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_y, y, n * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_row, row, (n + 1) * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_col, col, nnz * sizeof(int), hipMemcpyHostToDevice);

    // Define block and grid sizes
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;

    // Launch HIP kernel
    hipLaunchKernelGGL(spmv_kernel, gridSize, blockSize, 0, 0, n, nnz, d_val, d_row, d_col, d_x, d_y);

    // Copy result back to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_val);
    hipFree(d_x);
    hipFree(d_y);
    hipFree(d_row);
    hipFree(d_col);
}

__global__ void spmv_kernel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0;
        for (int j = row[i]; j < row[i + 1]; ++j) {
            sum += val[j] * x[col[j]];
        }
        y[i] = sum;
    }
}

Code Block 7:
void spmv_parallel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y) {
    int i, j;

    #pragma omp parallel for private(i, j)
    for (i = 0; i < n; i++) {
        y[i] = 0.0;
        for (j = row[i]; j < row[i + 1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

__global__ void spmv_kernel(int n, const float *val, const int *row, const int *col, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0;
        for (int j = row[i]; j < row[i + 1]; j++) {
            sum += val[j] * x[col[j]];
        }
        y[i] = sum;
    }
}

Code Block 8:
extern "C" void spmv_parallel(int n, int nnz, const float val[], const int row[], const int col[], const float x[], float y[])
{
    // Allocate device memory
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;
    CHECK_HIP_ERROR(hipMalloc(&d_val, nnz * sizeof(float)));
    CHECK_HIP_ERROR(hipMalloc(&d_row, (n + 1) * sizeof(int)));
    CHECK_HIP_ERROR(hipMalloc(&d_col, nnz * sizeof(int)));
    CHECK_HIP_ERROR(hipMalloc(&d_x, n * sizeof(float)));
    CHECK_HIP_ERROR(hipMalloc(&d_y, n * sizeof(float)));
    
    // Copy data to device
    CHECK_HIP_ERROR(hipMemcpy(d_val, val, nnz * sizeof(float), hipMemcpyHostToDevice));
    CHECK_HIP_ERROR(hipMemcpy(d_row, row, (n + 1) * sizeof(int), hipMemcpyHostToDevice));
    CHECK_HIP_ERROR(hipMemcpy(d_col, col, nnz * sizeof(int), hipMemcpyHostToDevice));
    CHECK_HIP_ERROR(hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice));
    
    // Determine thread block size and number of blocks
    int blockSize = 256;
    int numBlocks = (n + blockSize - 1) / blockSize;
    
    // Launch the kernel
    hipLaunchKernelGGL(spmv_kernel, dim3(numBlocks), dim3(blockSize), 0, 0, n, nnz, d_val, d_row, d_col, d_x, d_y);
    
    // Copy result back to host
    CHECK_HIP_ERROR(hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost));
    
    // Free device memory
    CHECK_HIP_ERROR(hipFree(d_val));
    CHECK_HIP_ERROR(hipFree(d_row));
    CHECK_HIP_ERROR(hipFree(d_col));
    CHECK_HIP_ERROR(hipFree(d_x));
    CHECK_HIP_ERROR(hipFree(d_y));
}

__global__ void spmv_kernel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i < n) {
        float temp = 0.0;
        for (int j = row[i]; j < row[i + 1]; j++) {
            temp += val[j] * x[col[j]];
        }
        y[i] = temp;
    }
}

Code Block 9:
void spmv_parallel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y) {
    int i, j;
    
    #pragma omp parallel for private(i, j) shared(n, nnz, val, row, col, x, y) default(none)
    for (i = 0; i < n; i++) {
        y[i] = 0.0;
        for (j = row[i]; j < row[i + 1]; j++) {
            y[i] += val[j] * x[col[j]];
        }
    }
}

__global__ void spmv_kernel(int n, const float *val, const int *row, const int *col, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0;
        for (int j = row[i]; j < row[i + 1]; j++) {
            sum += val[j] * x[col[j]];
        }
        y[i] = sum;
    }
}

Code Block 10:
extern "C" void spmv_parallel(int n, int nnz, const float *val, const int *row, const int *col, const float *x, float *y) {
    float *d_val, *d_x, *d_y;
    int *d_row, *d_col;

    // Allocate device memory
    hipMalloc((void **)&d_val, nnz * sizeof(float));
    hipMalloc((void **)&d_row, (n + 1) * sizeof(int));
    hipMalloc((void **)&d_col, nnz * sizeof(int));
    hipMalloc((void **)&d_x, n * sizeof(float));
    hipMalloc((void **)&d_y, n * sizeof(float));

    // Copy data to device
    hipMemcpy(d_val, val, nnz * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_row, row, (n + 1) * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_col, col, nnz * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_x, x, n * sizeof(float), hipMemcpyHostToDevice);

    // Launch Kernel
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;
    hipLaunchKernelGGL(spmv_kernel, dim3(gridSize), dim3(blockSize), 0, 0, n, d_val, d_row, d_col, d_x, d_y);

    // Copy result back to host
    hipMemcpy(y, d_y, n * sizeof(float), hipMemcpyDeviceToHost);

    // Free device memory
    hipFree(d_val);
    hipFree(d_row);
    hipFree(d_col);
    hipFree(d_x);
    hipFree(d_y);
}

__global__ void spmv_kernel(int n, const float *val, const int *row, const int *col, const float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float sum = 0.0f;
        for (int j = row[i]; j < row[i + 1]; ++j) {
            sum += val[j] * x[col[j]];
        }
        y[i] = sum;
    }
}

