Output 1:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in the OpenACC version, the gang vector clause is used to parallelize the outer two loops, and the reduction clause is used to parallelize the inner loop. The private clause is used to ensure that each thread has its own copy of the temp variable.
Also, note that the array indices are calculated manually in the OpenACC version, since OpenACC does not support assumed-shape arrays like Fortran does.
You can call this function from your Fortran code using the ISO_C_BINDING module:
use iso_c_binding
...
interface
  subroutine gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc) bind(c)
    use iso_c_binding
    integer(c_int), intent(in) :: m, n, k, lda, ldb, ldc
    real(c_double), intent(in) :: alpha, beta
    real(c_double), intent(in) :: a(lda, k), b(ldb, n)
    real(c_double), intent(inout) :: c(ldc, n)
  end subroutine gemm_parallel
end interface
...
call gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
This will allow you to call the C function from your Fortran code.


Output 2:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector collapse(2) private(temp)
  for (int j = 0; j < n; j++) {
    for (int i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (int l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
And here is the equivalent function in C with OpenMP:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (int j = 0; j < n; j++) {
    for (int i = 0; i < m; i++) {
      temp = 0.0;
      for (int l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in the OpenACC version, the #pragma acc loop reduction(+:temp) directive is used to perform a reduction on the temp variable, which is equivalent to the OpenMP reduction(+:temp) clause. Also, the gang vector clause is used to specify the parallelization strategy, and the collapse(2) clause is used to collapse the two nested loops into a single loop.


Output 3:
Here is the equivalent function in C with OpenACC:
#include <openacc.h>

void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C, arrays are row-major, so the indexing is different from Fortran. Also, the reduction clause is used to parallelize the inner loop, which is not possible in OpenMP.


Output 4:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector private(temp)
  for (j = 0; j < n; j++) {
    #pragma acc loop seq
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop seq
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
And here is the equivalent function in C with OpenMP:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma omp parallel for private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in the OpenACC version, the gang vector clause is used to parallelize the outer loop, and the seq clause is used to serialize the inner loops. In the OpenMP version, the parallel for directive is used to parallelize the outer loop, and the private clause is used to declare the loop indices and the temporary variable as private.


Output 5:
Here is the equivalent C code using OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop seq
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in OpenACC, the gang and vector clauses are used to parallelize the outer loops, while the seq clause is used to ensure that the inner loop is executed sequentially. Also, the private clause is used to declare private variables for each thread.
Also, note that in C, arrays are passed to functions as pointers, so the lda, ldb, and ldc parameters are not needed. Instead, the array is accessed using pointer arithmetic.
You can call this function using OpenACC's #pragma acc data directive to transfer the data to the device and execute the function on the device.
#pragma acc data copy(a[:m*k], b[:k*n], c[:m*n])
gemm_parallel(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
This will transfer the arrays a, b, and c to the device, execute the gemm_parallel function on the device, and then transfer the result back to the host.


Output 6:
Here is the equivalent function in C with OpenACC:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop private(i, j, l, temp) collapse(2)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in C, arrays are passed to functions as pointers, so we need to use pointer arithmetic to access the elements of the arrays. Also, the collapse(2) clause in the #pragma acc parallel loop directive tells the compiler to collapse the two nested loops into a single loop, which can improve parallelization efficiency.
As for the OpenACC directives, #pragma acc parallel loop specifies that the loop should be parallelized, and private(i, j, l, temp) specifies that these variables should be privatized for each thread. The reduction(+:temp) clause in the inner loop specifies that the temp variable should be reduced using the + operator, which is necessary for the dot product calculation.
I hope this helps! Let me know if you have any questions.


Output 7:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector private(temp)
  for (j = 0; j < n; j++) {
    #pragma acc loop vector
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop seq
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
And here is the equivalent function in C with OpenMP:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma omp parallel for private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in the OpenACC version, the #pragma acc routine seq directive is used to specify that the routine should be executed sequentially on the device. The #pragma acc parallel loop directive is used to specify that the loop should be parallelized, and the gang vector clause is used to specify the parallelization strategy. The private(temp) clause is used to specify that the temp variable should be privatized for each thread.
In the OpenMP version, the #pragma omp parallel for directive is used to specify that the loop should be parallelized, and the private(i, j, l, temp) clause is used to specify that the i, j, l, and temp variables should be privatized for each thread.


Output 8:
Here is the equivalent function in C with OpenACC:
#include <openacc.h>

void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop private(i, j, l, temp)
  for (j = 0; j < n; j++) {
    #pragma acc loop
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop reduction(+:temp)
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that I've assumed real(kind=8) in Fortran corresponds to double in C. Also, the lda, ldb, and ldc parameters are used to access the 2D arrays a, b, and c in column-major order, as in the Fortran code.
To compile this code with OpenACC, you'll need to use a compiler that supports OpenACC, such as GCC or PGI. For example:
gcc -fopenacc -o gemm_parallel gemm_parallel.c
This will generate an executable file gemm_parallel that can be run on a GPU or other accelerator device supported by OpenACC.


Output 9:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector collapse(3) present(a, b, c)
  for (j = 0; j < n; j++) {
    for (i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop seq
      for (l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that the OpenACC gang vector clause is used to parallelize the loop across multiple GPUs or accelerators, while the collapse(3) clause is used to collapse the three nested loops into a single loop. The present clause is used to specify that the arrays a, b, and c are present on the device.
Also, the #pragma acc loop seq directive is used to specify that the innermost loop should be executed sequentially, as it is not parallelizable.
You can call this function on the host by passing the appropriate arguments, like this:
int main() {
  int m = 100, n = 100, k = 100;
  double alpha = 1.0, beta = 1.0;
  double *a = (double *)malloc(m * k * sizeof(double));
  double *b = (double *)malloc(k * n * sizeof(double));
  double *c = (double *)malloc(m * n * sizeof(double));

  // Initialize a, b, and c

  #pragma acc data copyin(a[0:m*k], b[0:k*n], c[0:m*n])
  gemm_parallel(m, n, k, alpha, a, m, b, k, beta, c, m);
  #pragma acc wait

  // Use the result

  free(a);
  free(b);
  free(c);

  return 0;
}
This code allocates memory for the matrices a, b, and c on the host, initializes them, and then copies them to the device using the #pragma acc data directive. The gemm_parallel function is then called on the device, and the result is copied back to the host using the #pragma acc wait directive.


Output 10:
Here is the equivalent function in C with OpenACC:
#pragma acc routine seq
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma acc parallel loop gang vector collapse(2) private(temp)
  for (int j = 0; j < n; j++) {
    for (int i = 0; i < m; i++) {
      temp = 0.0;
      #pragma acc loop seq
      for (int l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
And here is the equivalent function in C with OpenMP:
void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i, j, l;
  double temp;
  #pragma omp parallel for private(i, j, l, temp) collapse(2)
  for (int j = 0; j < n; j++) {
    for (int i = 0; i < m; i++) {
      temp = 0.0;
      for (int l = 0; l < k; l++) {
        temp += a[i * lda + l] * b[l * ldb + j];
      }
      c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
    }
  }
}
Note that in the OpenACC version, I used #pragma acc routine seq to ensure that the loop is executed sequentially on the device, as OpenACC doesn't support reduction operations like OpenMP does. Also, I used gang vector collapse(2) to parallelize the loop over j and i simultaneously, and private(temp) to make sure each thread has its own copy of the temp variable. In the OpenMP version, I used collapse(2) to parallelize the loop over j and i simultaneously, and private(i, j, l, temp) to make sure each thread has its own copy of the loop indices and the temp variable.


