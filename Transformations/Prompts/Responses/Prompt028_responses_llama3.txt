Output 1:
Here is the equivalent code in C and HIP, and Fortran and OpenACC:
C and HIP
void jacobi_parallel(float *u, float *unew, int n, int niter) {
  int iter, i, j, k;
  for (iter = 0; iter < niter; iter++) {
    hipLaunchKernelGGL(jacobi_kernel, dim3(n, n, n), dim3(1, 1, 1), 0, 0, u, unew, n);
    float *tmp = u;
    u = unew;
    unew = tmp;
  }
}

__global__ void jacobi_kernel(float *u, float *unew, int n) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  int k = blockIdx.z * blockDim.z + threadIdx.z;
  if (i >= 2 && i < n - 1 && j >= 2 && j < n - 1 && k >= 2 && k < n - 1) {
    unew[i * n * n + j * n + k] = 0.125f * (u[(i - 1) * n * n + j * n + k] +
                                             u[(i + 1) * n * n + j * n + k] +
                                             u[i * n * n + (j - 1) * n + k] +
                                             u[i * n * n + (j + 1) * n + k] +
                                             u[i * n * n + j * n + (k - 1)] +
                                             u[i * n * n + j * n + (k + 1)] +
                                             u[i * n * n + j * n + k]);
  }
}
Fortran and OpenACC
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
  do iter = 1, niter
    !$ACC PARALLEL LOOP COLLAPSE(3) PRIVATE(i, j, k)
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2, n - 1
          unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
            + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
        end do
      end do
    end do
    !$ACC END PARALLEL LOOP
    u(:, :, :) = unew(:, :, :)
  end do
end subroutine jacobi_parallel
Note that in the HIP version, I used a temporary pointer swap to swap the u and unew arrays, as HIP does not support array swaps in the same way as Fortran. I also used float instead of real(kind=8) to match the C data type. You may need to adjust the data type and precision to match your specific use case.


Output 2:
Here is the equivalent code in C and HIP, and Fortran and OpenACC:
C and HIP
#include <hip/hip_runtime.h>

void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i, j, k, iter;
  for (iter = 1; iter <= niter; iter++) {
    hipLaunchKernelGGL(jacobi_kernel, dim3(n-2, n-2, n-2), dim3(1, 1, 1), 0, 0,
                       u, unew, n);
    double *tmp = u; u = unew; unew = tmp;  // swap pointers
  }
}

__global__ void jacobi_kernel(double *u, double *unew, int n) {
  int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
  int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
  int k = blockIdx.z * blockDim.z + threadIdx.z + 1;
  if (i < n-1 && j < n-1 && k < n-1) {
    unew[i * n * n + j * n + k] = 0.125 * (u[(i-1) * n * n + j * n + k] +
                                            u[(i+1) * n * n + j * n + k] +
                                            u[i * n * n + (j-1) * n + k] +
                                            u[i * n * n + (j+1) * n + k] +
                                            u[i * n * n + j * n + (k-1)] +
                                            u[i * n * n + j * n + (k+1)] +
                                            u[i * n * n + j * n + k]);
  }
}
Fortran and OpenACC
subroutine jacobi_parallel(u, unew, n, niter)
    implicit none
    integer, intent(in) :: n, niter
    real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
    integer :: i, j, k, iter
    do iter = 1, niter
      !$ACC PARALLEL LOOP COLLAPSE(3) PRIVATE(i, j, k)
      do k = 2, n - 1
        do j = 2, n - 1
          do i = 2, n - 1
            unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
              + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
          end do
        end do
      end do
      !$ACC END PARALLEL LOOP
      u(:, :, :) = unew(:, :, :)
    end do
end subroutine jacobi_parallel
Note that in the HIP version, I've used a kernel launch with block and grid dimensions to parallelize the computation, and in the Fortran and OpenACC version, I've used the !$ACC PARALLEL LOOP directive to parallelize the computation. Also, in the HIP version, I've used a temporary pointer swap to swap the u and unew arrays, whereas in the Fortran and OpenACC version, I've used the u(:, :, :) = unew(:, :, :) assignment to swap the arrays.


Output 3:
Here are the implementations of the Jacobi operation in C, HIP, Fortran, and OpenACC:
C:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i, j, k, iter;
  for (iter = 0; iter < niter; iter++) {
    #pragma omp parallel for collapse(3) private(i, j, k)
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[i * n * n + j * n + k] = 0.125 * (u[(i - 1) * n * n + j * n + k] +
              u[(i + 1) * n * n + j * n + k] + u[i * n * n + (j - 1) * n + k] +
              u[i * n * n + (j + 1) * n + k] + u[i * n * n + j * n + (k - 1)] +
              u[i * n * n + j * n + (k + 1)] + u[i * n * n + j * n + k]);
        }
      }
    }
    for (k = 0; k < n * n * n; k++) {
      u[k] = unew[k];
    }
  }
}
HIP:
__global__ void jacobi_kernel(double *u, double *unew, int n) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  int k = blockIdx.z * blockDim.z + threadIdx.z;
  if (i >= 1 && i < n - 1 && j >= 1 && j < n - 1 && k >= 1 && k < n - 1) {
    unew[i * n * n + j * n + k] = 0.125 * (u[(i - 1) * n * n + j * n + k] +
        u[(i + 1) * n * n + j * n + k] + u[i * n * n + (j - 1) * n + k] +
        u[i * n * n + (j + 1) * n + k] + u[i * n * n + j * n + (k - 1)] +
        u[i * n * n + j * n + (k + 1)] + u[i * n * n + j * n + k]);
  }
}

void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int blockSize = 16;
  int numBlocks = (n + blockSize - 1) / blockSize;
  for (int iter = 0; iter < niter; iter++) {
    hipLaunchKernelGGL(jacobi_kernel, dim3(numBlocks, numBlocks, numBlocks), dim3(blockSize, blockSize, blockSize), 0, 0, u, unew, n);
    hipDeviceSynchronize();
    for (int k = 0; k < n * n * n; k++) {
      u[k] = unew[k];
    }
  }
}
Fortran with OpenACC:
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
  do iter = 1, niter
    !$ACC PARALLEL LOOP COLLAPSE(3) PRIVATE(i, j, k)
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2, n - 1
          unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
            + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
        end do
      end do
    end do
    !$ACC END PARALLEL LOOP
    u(:, :, :) = unew(:, :, :)
  end do
end subroutine jacobi_parallel
Note that the HIP implementation uses a GPU kernel to perform


Output 4:
Here are the implementations of the Jacobi operation in C, HIP, Fortran, and OpenACC:
C:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i, j, k, iter;
 #pragma omp parallel for collapse(3) private(i, j, k)
  for (iter = 0; iter < niter; iter++) {
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[i * n * n + j * n + k] = 0.125 * (u[(i - 1) * n * n + j * n + k] +
              u[(i + 1) * n * n + j * n + k] + u[i * n * n + (j - 1) * n + k] +
              u[i * n * n + (j + 1) * n + k] + u[i * n * n + j * n + (k - 1)] +
              u[i * n * n + j * n + (k + 1)] + u[i * n * n + j * n + k]);
        }
      }
    }
  }
  for (i = 0; i < n * n * n; i++) {
    u[i] = unew[i];
  }
}
HIP:
__global__ void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  int k = blockIdx.z * blockDim.z + threadIdx.z;
  if (i >= n || j >= n || k >= n) return;
  int idx = i * n * n + j * n + k;
  for (int iter = 0; iter < niter; iter++) {
    unew[idx] = 0.125 * (u[idx - n * n - n - 1] + u[idx - n * n - n + 1] +
        u[idx - n * n - 1] + u[idx - n * n + 1] + u[idx - n - n] +
        u[idx - n + n] + u[idx - n] + u[idx + n] + u[idx]);
  }
  u[idx] = unew[idx];
}
Fortran:
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
!$OMP PARALLEL DO COLLAPSE(3) PRIVATE(i, j, k)
  do iter = 1, niter
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2, n - 1
          unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
            + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
        end do
      end do
    end do
  end do
!$OMP END PARALLEL DO
  u(:, :, :) = unew(:, :, :)
end subroutine jacobi_parallel
OpenACC:
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
!$ACC PARALLEL LOOP COLLAPSE(3) PRIVATE(i, j, k)
  do iter = 1, niter
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2, n - 1
          unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
            + u(i, j+1,


Output 5:
Here are the implementations of the Jacobi operation in C, HIP, Fortran, and OpenACC:
C:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i, j, k, iter;
 #pragma omp parallel for collapse(3) private(i, j, k)
  for (iter = 0; iter < niter; iter++) {
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[i * n * n + j * n + k] = 0.125 * (
            u[(i - 1) * n * n + j * n + k] + u[(i + 1) * n * n + j * n + k] +
            u[i * n * n + (j - 1) * n + k] + u[i * n * n + (j + 1) * n + k] +
            u[i * n * n + j * n + (k - 1)] + u[i * n * n + j * n + (k + 1)] +
            u[i * n * n + j * n + k]);
        }
      }
    }
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          u[i * n * n + j * n + k] = unew[i * n * n + j * n + k];
        }
      }
    }
  }
}
HIP:
__global__ void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  int k = blockIdx.z * blockDim.z + threadIdx.z;
  if (i >= 1 && i < n - 1 && j >= 1 && j < n - 1 && k >= 1 && k < n - 1) {
    for (int iter = 0; iter < niter; iter++) {
      unew[i * n * n + j * n + k] = 0.125 * (
        u[(i - 1) * n * n + j * n + k] + u[(i + 1) * n * n + j * n + k] +
        u[i * n * n + (j - 1) * n + k] + u[i * n * n + (j + 1) * n + k] +
        u[i * n * n + j * n + (k - 1)] + u[i * n * n + j * n + (k + 1)] +
        u[i * n * n + j * n + k]);
    }
    u[i * n * n + j * n + k] = unew[i * n * n + j * n + k];
  }
}
Fortran with OpenACC:
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
  !$ACC PARALLEL LOOP COLLAPSE(3) PRIVATE(i, j, k)
  do iter = 1, niter
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2, n - 1
          unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
            + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
        end do
      end do
    end do
    !$ACC END PARALLEL LOOP
    u(:, :, :) = unew(:, :, :)
  end do
end subroutine jacobi_parallel
Note that in the HIP implementation, I assumed a 3D block and grid layout to parallelize the computation. You may need to adjust the block and grid sizes based on your specific hardware and problem size.


Output 6:
Here are the implementations of the Jacobi operation in C, HIP, Fortran, and OpenACC:
C:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
  int i, j, k, iter;
  #pragma omp parallel for collapse(3) private(i, j, k)
  for (iter = 0; iter < niter; iter++) {
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[i][j][k] = 0.125 * (u[i-1][j][k] + u[i+1][j][k] + u[i][j-1][k] +
                                    u[i][j+1][k] + u[i][j][k-1] + u[i][j][k+1] + u[i][j][k]);
        }
      }
    }
  }
  for (k = 0; k < n; k++) {
    for (j = 0; j < n; j++) {
      for (i = 0; i < n; i++) {
        u[i][j][k] = unew[i][j][k];
      }
    }
  }
}
HIP:
__global__ void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  int k = blockIdx.z * blockDim.z + threadIdx.z;
  if (i >= n || j >= n || k >= n) return;
  int iter;
  for (iter = 0; iter < niter; iter++) {
    unew[i + j * n + k * n * n] = 0.125 * (u[(i-1) + j * n + k * n * n] + u[(i+1) + j * n + k * n * n] +
                                        u[i + (j-1) * n + k * n * n] + u[i + (j+1) * n + k * n * n] +
                                        u[i + j * n + (k-1) * n * n] + u[i + j * n + (k+1) * n * n] +
                                        u[i + j * n + k * n * n]);
  }
  u[i + j * n + k * n * n] = unew[i + j * n + k * n * n];
}
Fortran (with OpenACC):
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
  !$ACC PARALLEL LOOP COLLAPSE(3) PRIVATE(i, j, k)
  do iter = 1, niter
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2, n - 1
          unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
            + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
        end do
      end do
    end do
  end do
  !$ACC END PARALLEL LOOP
  u(:, :, :) = unew(:, :, :)
end subroutine jacobi_parallel
Note that the HIP implementation uses a 3D grid of blocks, with each block processing a subset of the data. The Fortran implementation uses OpenACC directives to parallelize the loop. The C implementation uses OpenMP directives to parallelize the loop.


Output 7:
Here are the implementations of the Jacobi operation in C, HIP, Fortran, and OpenACC:
C:
void jacobi_parallel(double ***u, double ***unew, int n, int niter) {
  int i, j, k, iter;
  for (iter = 0; iter < niter; iter++) {
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[i][j][k] = 0.125 * (u[i-1][j][k] + u[i+1][j][k] + u[i][j-1][k] +
                                     u[i][j+1][k] + u[i][j][k-1] + u[i][j][k+1] + u[i][j][k]);
        }
      }
    }
    for (k = 0; k < n; k++) {
      for (j = 0; j < n; j++) {
        for (i = 0; i < n; i++) {
          u[i][j][k] = unew[i][j][k];
        }
      }
    }
  }
}
HIP:
__global__ void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  int k = blockIdx.z * blockDim.z + threadIdx.z;
  if (i >= 1 && i < n - 1 && j >= 1 && j < n - 1 && k >= 1 && k < n - 1) {
    unew[i * n * n + j * n + k] = 0.125 * (u[(i-1) * n * n + j * n + k] + u[(i+1) * n * n + j * n + k] +
                                           u[i * n * n + (j-1) * n + k] + u[i * n * n + (j+1) * n + k] +
                                           u[i * n * n + j * n + (k-1)] + u[i * n * n + j * n + (k+1)] + u[i * n * n + j * n + k]);
  }
}
Fortran:
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
  do iter = 1, niter
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2, n - 1
          unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
            + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
        end do
      end do
    end do
    u(:, :, :) = unew(:, :, :)
  end do
end subroutine jacobi_parallel
OpenACC:
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
  do iter = 1, niter
    !$ACC PARALLEL LOOP COLLAPSE(3) PRIVATE(i, j, k)
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2, n - 1
          unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
            + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
        end do
      end do
    end


Output 8:
Here are the implementations of the Jacobi operation in C, HIP, Fortran, and OpenACC:
C:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i, j, k, iter;
  for (iter = 0; iter < niter; iter++) {
    #pragma omp parallel for collapse(3) private(i, j, k)
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[i * n * n + j * n + k] = 0.125 * (
            u[(i - 1) * n * n + j * n + k] + u[(i + 1) * n * n + j * n + k] +
            u[i * n * n + (j - 1) * n + k] + u[i * n * n + (j + 1) * n + k] +
            u[i * n * n + j * n + (k - 1)] + u[i * n * n + j * n + (k + 1)] +
            u[i * n * n + j * n + k]);
        }
      }
    }
    for (k = 0; k < n * n * n; k++) {
      u[k] = unew[k];
    }
  }
}
HIP:
__global__ void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  int k = blockIdx.z * blockDim.z + threadIdx.z;
  if (i >= 1 && i < n - 1 && j >= 1 && j < n - 1 && k >= 1 && k < n - 1) {
    for (int iter = 0; iter < niter; iter++) {
      unew[i * n * n + j * n + k] = 0.125 * (
        u[(i - 1) * n * n + j * n + k] + u[(i + 1) * n * n + j * n + k] +
        u[i * n * n + (j - 1) * n + k] + u[i * n * n + (j + 1) * n + k] +
        u[i * n * n + j * n + (k - 1)] + u[i * n * n + j * n + (k + 1)] +
        u[i * n * n + j * n + k]);
    }
  }
}
Fortran:
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
  do iter = 1, niter
    !$ACC PARALLEL LOOP COLLAPSE(3) PRIVATE(i, j, k)
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2, n - 1
          unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
            + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
        end do
      end do
    end do
    !$ACC END PARALLEL LOOP
    u(:, :, :) = unew(:, :, :)
  end do
end subroutine jacobi_parallel
OpenACC:
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
  do iter = 1, niter
    !$ACC PARALLEL LOOP COLLAPSE(3) PRIVATE(i, j, k)
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2,


Output 9:
Here are the implementations of the Jacobi operation in C, HIP, Fortran, and OpenACC:
C:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i, j, k, iter;
 #pragma omp parallel for collapse(3) private(i, j, k)
  for (iter = 0; iter < niter; iter++) {
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[i * n * n + j * n + k] = 0.125 * (u[(i - 1) * n * n + j * n + k] +
              u[(i + 1) * n * n + j * n + k] + u[i * n * n + (j - 1) * n + k] +
              u[i * n * n + (j + 1) * n + k] + u[i * n * n + j * n + (k - 1)] +
              u[i * n * n + j * n + (k + 1)] + u[i * n * n + j * n + k]);
        }
      }
    }
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          u[i * n * n + j * n + k] = unew[i * n * n + j * n + k];
        }
      }
    }
  }
}
HIP:
__global__ void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  int k = blockIdx.z * blockDim.z + threadIdx.z;
  if (i >= 1 && i < n - 1 && j >= 1 && j < n - 1 && k >= 1 && k < n - 1) {
    for (int iter = 0; iter < niter; iter++) {
      unew[i * n * n + j * n + k] = 0.125 * (u[(i - 1) * n * n + j * n + k] +
          u[(i + 1) * n * n + j * n + k] + u[i * n * n + (j - 1) * n + k] +
          u[i * n * n + (j + 1) * n + k] + u[i * n * n + j * n + (k - 1)] +
          u[i * n * n + j * n + (k + 1)] + u[i * n * n + j * n + k]);
    }
    u[i * n * n + j * n + k] = unew[i * n * n + j * n + k];
  }
}
Fortran:
subroutine jacobi_parallel(u, unew, n, niter)
    implicit none
    integer, intent(in) :: n, niter
    real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
    integer :: i, j, k, iter
    !$OMP PARALLEL DO COLLAPSE(3) PRIVATE(i, j, k)
    do iter = 1, niter
      do k = 2, n - 1
        do j = 2, n - 1
          do i = 2, n - 1
            unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
              + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
          end do
        end do
      end do
      u(:, :, :) = unew(:, :, :)
    end do
end subroutine jacobi_parallel
OpenACC:
subroutine jacobi_parallel(u, unew, n, niter)
    implicit none
    integer, intent(in) :: n, niter
    real(kind=8), dimension(n, n, n), intent(inout)


Output 10:
Here are the implementations of the Jacobi operation in C, HIP, Fortran, and OpenACC:
C:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i, j, k, iter;
  #pragma omp parallel for collapse(3) private(i, j, k)
  for (iter = 0; iter < niter; iter++) {
    for (k = 1; k < n - 1; k++) {
      for (j = 1; j < n - 1; j++) {
        for (i = 1; i < n - 1; i++) {
          unew[i * n * n + j * n + k] = 0.125 * (u[(i - 1) * n * n + j * n + k] +
              u[(i + 1) * n * n + j * n + k] + u[i * n * n + (j - 1) * n + k] +
              u[i * n * n + (j + 1) * n + k] + u[i * n * n + j * n + (k - 1)] +
              u[i * n * n + j * n + (k + 1)] + u[i * n * n + j * n + k]);
        }
      }
    }
    for (k = 0; k < n * n * n; k++) {
      u[k] = unew[k];
    }
  }
}
HIP:
__global__ void jacobi_parallel(double *u, double *unew, int n, int niter) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  int k = blockIdx.z * blockDim.z + threadIdx.z;
  if (i >= n || j >= n || k >= n) return;
  int idx = i * n * n + j * n + k;
  for (int iter = 0; iter < niter; iter++) {
    unew[idx] = 0.125 * (u[idx - n * n - n - 1] + u[idx - n * n - n + 1] +
        u[idx - n * n - 1] + u[idx - n * n + 1] + u[idx - n - 1] +
        u[idx - n + 1] + u[idx - 1] + u[idx + 1] + u[idx + n - 1] +
        u[idx + n + 1] + u[idx + n * n - n - 1] + u[idx + n * n - n + 1] +
        u[idx + n * n - 1] + u[idx + n * n + 1] + u[idx + n * n + n - 1] +
        u[idx + n * n + n + 1]);
  }
  u[idx] = unew[idx];
}
Fortran with OpenACC:
subroutine jacobi_parallel(u, unew, n, niter)
  implicit none
  integer, intent(in) :: n, niter
  real(kind=8), dimension(n, n, n), intent(inout) :: u, unew
  integer :: i, j, k, iter
  !$ACC PARALLEL LOOP COLLAPSE(3) PRIVATE(i, j, k)
  do iter = 1, niter
    do k = 2, n - 1
      do j = 2, n - 1
        do i = 2, n - 1
          unew(i, j, k) = 0.125 * (u(i-1, j, k) + u(i+1, j, k) + u(i, j-1, k) &
            + u(i, j+1, k) + u(i, j, k-1) + u(i, j, k+1) + u(i, j, k))
        end do
      end do
    end do
    !$ACC END PARALLEL LOOP
    u(:, :, :) = unew(:, :, :)
  end do
end subroutine jacobi_parallel
Note that the HIP implementation uses a 3D block and grid to parallelize the computation, while the OpenACC implementation uses a parallel loop collapse clause to parallelize the computation. The C implementation uses OpenMP to parallelize the computation.


