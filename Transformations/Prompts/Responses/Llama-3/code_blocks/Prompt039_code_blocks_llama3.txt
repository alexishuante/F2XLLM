--------------- Code Block 1 ---------------
__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) {
    float sum = 0.0;
    for (int j = row[i]; j < row[i+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[i] = sum;
  }
}

void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  spmv_kernel<<<numBlocks, blockSize>>>(n, nnz, val, row, col, x, y);
  cudaDeviceSynchronize();
}

--------------- Code Block 2 ---------------
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) {
    float sum = 0.0;
    for (int j = row[i]; j < row[i+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[i] = sum;
  }
}

void spmv_cuda(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  float *d_val, *d_x, *d_y;
  int *d_row, *d_col;

  cudaMalloc((void **)&d_val, nnz * sizeof(float));
  cudaMalloc((void **)&d_x, n * sizeof(float));
  cudaMalloc((void **)&d_y, n * sizeof(float));
  cudaMalloc((void **)&d_row, (n+1) * sizeof(int));
  cudaMalloc((void **)&d_col, nnz * sizeof(int));

  cudaMemcpy(d_val, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_row, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
  cudaMemcpy(d_col, col, nnz * sizeof(int), cudaMemcpyHostToDevice);

  spmv_parallel<<<numBlocks, blockSize>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);

  cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);

  cudaFree(d_val);
  cudaFree(d_x);
  cudaFree(d_y);
  cudaFree(d_row);
  cudaFree(d_col);
}

--------------- Code Block 3 ---------------
__global__ void spmv_kernel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) {
    float sum = 0.0;
    for (int j = row[i]; j < row[i+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[i] = sum;
  }
}

void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  spmv_kernel<<<numBlocks, blockSize>>>(n, nnz, val, row, col, x, y);
  cudaDeviceSynchronize();
}

--------------- Code Block 4 ---------------
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) {
    float sum = 0.0;
    for (int j = row[i]; j < row[i+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[i] = sum;
  }
}

void spmv_wrapper(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  float *d_val, *d_x, *d_y;
  int *d_row, *d_col;
  cudaMalloc((void **)&d_val, nnz * sizeof(float));
  cudaMalloc((void **)&d_x, n * sizeof(float));
  cudaMalloc((void **)&d_y, n * sizeof(float));
  cudaMalloc((void **)&d_row, (n+1) * sizeof(int));
  cudaMalloc((void **)&d_col, nnz * sizeof(int));
  cudaMemcpy(d_val, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_row, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
  cudaMemcpy(d_col, col, nnz * sizeof(int), cudaMemcpyHostToDevice);
  spmv_parallel<<<numBlocks, blockSize>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);
  cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);
  cudaFree(d_val);
  cudaFree(d_x);
  cudaFree(d_y);
  cudaFree(d_row);
  cudaFree(d_col);
}

--------------- Code Block 5 ---------------
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) {
    float sum = 0.0;
    for (int j = row[i]; j < row[i+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[i] = sum;
  }
}

void spmv_parallel_wrapper(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  float *d_val, *d_x, *d_y;
  int *d_row, *d_col;
  cudaMalloc((void **)&d_val, nnz * sizeof(float));
  cudaMalloc((void **)&d_x, n * sizeof(float));
  cudaMalloc((void **)&d_y, n * sizeof(float));
  cudaMalloc((void **)&d_row, (n+1) * sizeof(int));
  cudaMalloc((void **)&d_col, nnz * sizeof(int));
  cudaMemcpy(d_val, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_row, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
  cudaMemcpy(d_col, col, nnz * sizeof(int), cudaMemcpyHostToDevice);
  spmv_parallel<<<numBlocks, blockSize>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);
  cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);
  cudaFree(d_val); cudaFree(d_x); cudaFree(d_y); cudaFree(d_row); cudaFree(d_col);
}

--------------- Code Block 6 ---------------
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) {
    float sum = 0.0;
    for (int j = row[i]; j < row[i+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[i] = sum;
  }
}

void spmv_parallel_wrapper(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  float *val_d, *x_d, *y_d;
  int *row_d, *col_d;

  cudaMalloc((void **)&val_d, nnz * sizeof(float));
  cudaMalloc((void **)&x_d, n * sizeof(float));
  cudaMalloc((void **)&y_d, n * sizeof(float));
  cudaMalloc((void **)&row_d, (n+1) * sizeof(int));
  cudaMalloc((void **)&col_d, nnz * sizeof(int));

  cudaMemcpy(val_d, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(x_d, x, n * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(row_d, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
  cudaMemcpy(col_d, col, nnz * sizeof(int), cudaMemcpyHostToDevice);

  spmv_parallel<<<numBlocks, blockSize>>>(n, nnz, val_d, row_d, col_d, x_d, y_d);

  cudaMemcpy(y, y_d, n * sizeof(float), cudaMemcpyDeviceToHost);

  cudaFree(val_d);
  cudaFree(x_d);
  cudaFree(y_d);
  cudaFree(row_d);
  cudaFree(col_d);
}

--------------- Code Block 7 ---------------
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) {
    float sum = 0.0;
    for (int j = row[i]; j < row[i+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[i] = sum;
  }
}

void spmv_parallel_wrapper(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  float *val_d, *x_d, *y_d;
  int *row_d, *col_d;

  cudaMalloc((void **)&val_d, nnz * sizeof(float));
  cudaMalloc((void **)&x_d, n * sizeof(float));
  cudaMalloc((void **)&y_d, n * sizeof(float));
  cudaMalloc((void **)&row_d, (n+1) * sizeof(int));
  cudaMalloc((void **)&col_d, nnz * sizeof(int));

  cudaMemcpy(val_d, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(x_d, x, n * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(row_d, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
  cudaMemcpy(col_d, col, nnz * sizeof(int), cudaMemcpyHostToDevice);

  spmv_parallel<<<numBlocks, blockSize>>>(n, nnz, val_d, row_d, col_d, x_d, y_d);

  cudaMemcpy(y, y_d, n * sizeof(float), cudaMemcpyDeviceToHost);

  cudaFree(val_d);
  cudaFree(x_d);
  cudaFree(y_d);
  cudaFree(row_d);
  cudaFree(col_d);
}

--------------- Code Block 8 ---------------
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) {
    float sum = 0.0;
    for (int j = row[i]; j < row[i+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[i] = sum;
  }
}

void spmv_cuda(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  float *val_d, *x_d, *y_d;
  int *row_d, *col_d;

  cudaMalloc((void **)&val_d, nnz * sizeof(float));
  cudaMalloc((void **)&x_d, n * sizeof(float));
  cudaMalloc((void **)&y_d, n * sizeof(float));
  cudaMalloc((void **)&row_d, (n+1) * sizeof(int));
  cudaMalloc((void **)&col_d, nnz * sizeof(int));

  cudaMemcpy(val_d, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(x_d, x, n * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(row_d, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
  cudaMemcpy(col_d, col, nnz * sizeof(int), cudaMemcpyHostToDevice);

  spmv_parallel<<<numBlocks, blockSize>>>(n, nnz, val_d, row_d, col_d, x_d, y_d);

  cudaMemcpy(y, y_d, n * sizeof(float), cudaMemcpyDeviceToHost);

  cudaFree(val_d); cudaFree(x_d); cudaFree(y_d); cudaFree(row_d); cudaFree(col_d);
}

--------------- Code Block 9 ---------------
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) {
    float sum = 0.0;
    for (int j = row[i]; j < row[i+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[i] = sum;
  }
}

void spmv_parallel_wrapper(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  float *d_val, *d_x, *d_y;
  int *d_row, *d_col;
  cudaMalloc((void **)&d_val, nnz * sizeof(float));
  cudaMalloc((void **)&d_x, n * sizeof(float));
  cudaMalloc((void **)&d_y, n * sizeof(float));
  cudaMalloc((void **)&d_row, (n+1) * sizeof(int));
  cudaMalloc((void **)&d_col, nnz * sizeof(int));
  cudaMemcpy(d_val, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_x, x, n * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_row, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
  cudaMemcpy(d_col, col, nnz * sizeof(int), cudaMemcpyHostToDevice);
  spmv_parallel<<<numBlocks, blockSize>>>(n, nnz, d_val, d_row, d_col, d_x, d_y);
  cudaMemcpy(y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);
  cudaFree(d_val); cudaFree(d_x); cudaFree(d_y); cudaFree(d_row); cudaFree(d_col);
}

--------------- Code Block 10 ---------------
__global__ void spmv_parallel(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < n) {
    float sum = 0.0;
    for (int j = row[i]; j < row[i+1]; j++) {
      sum += val[j] * x[col[j]];
    }
    y[i] = sum;
  }
}

void spmv_parallel_wrapper(int n, int nnz, float *val, int *row, int *col, float *x, float *y) {
  int blockSize = 256;
  int numBlocks = (n + blockSize - 1) / blockSize;
  float *val_d, *x_d, *y_d;
  int *row_d, *col_d;
  cudaMalloc((void **)&val_d, nnz * sizeof(float));
  cudaMalloc((void **)&x_d, n * sizeof(float));
  cudaMalloc((void **)&y_d, n * sizeof(float));
  cudaMalloc((void **)&row_d, (n+1) * sizeof(int));
  cudaMalloc((void **)&col_d, nnz * sizeof(int));
  cudaMemcpy(val_d, val, nnz * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(x_d, x, n * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(row_d, row, (n+1) * sizeof(int), cudaMemcpyHostToDevice);
  cudaMemcpy(col_d, col, nnz * sizeof(int), cudaMemcpyHostToDevice);
  spmv_parallel<<<numBlocks, blockSize>>>(n, nnz, val_d, row_d, col_d, x_d, y_d);
  cudaMemcpy(y, y_d, n * sizeof(float), cudaMemcpyDeviceToHost);
  cudaFree(val_d); cudaFree(x_d); cudaFree(y_d); cudaFree(row_d); cudaFree(col_d);
}

