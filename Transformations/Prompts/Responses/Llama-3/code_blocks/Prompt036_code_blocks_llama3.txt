--------------- Code Block 1 ---------------
__global__ void gemm_kernel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    float temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

void gemm_parallel(int m, int n, int k, float alpha, float *a, int lda, float *b, int ldb, float beta, float *c, int ldc) {
  int i, j, l;
  float temp;
  cudaError_t err;

  // Allocate memory on the GPU
  float *a_gpu, *b_gpu, *c_gpu;
  err = cudaMalloc((void **)&a_gpu, lda * k * sizeof(float));
  err = cudaMalloc((void **)&b_gpu, ldb * n * sizeof(float));
  err = cudaMalloc((void **)&c_gpu, ldc * n * sizeof(float));

  // Copy data from host to GPU
  err = cudaMemcpy(a_gpu, a, lda * k * sizeof(float), cudaMemcpyHostToDevice);
  err = cudaMemcpy(b_gpu, b, ldb * n * sizeof(float), cudaMemcpyHostToDevice);
  err = cudaMemcpy(c_gpu, c, ldc * n * sizeof(float), cudaMemcpyHostToDevice);

  // Launch kernel
  dim3 blockSize(16, 16);
  dim3 gridSize((m + blockSize.x - 1) / blockSize.x, (n + blockSize.y - 1) / blockSize.y);
  gemm_kernel<<<gridSize, blockSize>>>(m, n, k, alpha, a_gpu, lda, b_gpu, ldb, beta, c_gpu, ldc);

  // Copy result from GPU to host
  err = cudaMemcpy(c, c_gpu, ldc * n * sizeof(float), cudaMemcpyDeviceToHost);

  // Free memory on the GPU
  err = cudaFree(a_gpu);
  err = cudaFree(b_gpu);
  err = cudaFree(c_gpu);
}

--------------- Code Block 2 ---------------
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  double temp = 0.0;

  if (i < m && j < n) {
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

void gemm_cuda(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = ceil(max(m, n) / blockSize);

  dim3 dimBlock(blockSize, blockSize);
  dim3 dimGrid(numBlocks, numBlocks);

  double *a_d, *b_d, *c_d;

  cudaMalloc((void **)&a_d, lda * k * sizeof(double));
  cudaMalloc((void **)&b_d, ldb * n * sizeof(double));
  cudaMalloc((void **)&c_d, ldc * n * sizeof(double));

  cudaMemcpy(a_d, a, lda * k * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(b_d, b, ldb * n * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(c_d, c, ldc * n * sizeof(double), cudaMemcpyHostToDevice);

  gemm_parallel<<<dimGrid, dimBlock>>>(m, n, k, alpha, a_d, lda, b_d, ldb, beta, c_d, ldc);

  cudaMemcpy(c, c_d, ldc * n * sizeof(double), cudaMemcpyDeviceToHost);

  cudaFree(a_d);
  cudaFree(b_d);
  cudaFree(c_d);
}

--------------- Code Block 3 ---------------
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  double temp = 0.0;

  if (i < m && j < n) {
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

void launch_gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = ceil(m / (float)blockSize);

  dim3 dimBlock(blockSize, blockSize);
  dim3 dimGrid(numBlocks, numBlocks);

  gemm_parallel<<<dimGrid, dimBlock>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
  cudaDeviceSynchronize();
}

--------------- Code Block 4 ---------------
__global__ void gemm_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;

  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = (m + blockSize - 1) / blockSize;

  dim3 blockDim(blockSize, blockSize);
  dim3 gridDim(numBlocks, numBlocks);

  gemm_kernel<<<gridDim, blockDim>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);

  cudaDeviceSynchronize();
}

--------------- Code Block 5 ---------------
__global__ void gemm_kernel(int m, int n, int k, double alpha, double *A, int lda, double *B, int ldb, double beta, double *C, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;

  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += A[i * lda + l] * B[l * ldb + j];
    }
    C[i * ldc + j] = alpha * temp + beta * C[i * ldc + j];
  }
}

void gemm_parallel(int m, int n, int k, double alpha, double *A, int lda, double *B, int ldb, double beta, double *C, int ldc) {
  int blockSize = 16;
  int numBlocks = ceil(m / blockSize) * ceil(n / blockSize);

  dim3 dimBlock(blockSize, blockSize);
  dim3 dimGrid(numBlocks, numBlocks);

  gemm_kernel<<<dimGrid, dimBlock>>>(m, n, k, alpha, A, lda, B, ldb, beta, C, ldc);

  cudaDeviceSynchronize();
}

--------------- Code Block 6 ---------------
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  if (i < m && j < n) {
    double temp = 0.0;
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

void gemm_parallel_host(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = (m + blockSize - 1) / blockSize;
  dim3 blockDim(blockSize, blockSize);
  dim3 gridDim(numBlocks, numBlocks);
  double *a_d, *b_d, *c_d;
  cudaMalloc((void **)&a_d, lda * k * sizeof(double));
  cudaMalloc((void **)&b_d, ldb * n * sizeof(double));
  cudaMalloc((void **)&c_d, ldc * n * sizeof(double));
  cudaMemcpy(a_d, a, lda * k * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(b_d, b, ldb * n * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(c_d, c, ldc * n * sizeof(double), cudaMemcpyHostToDevice);
  gemm_parallel<<<gridDim, blockDim>>>(m, n, k, alpha, a_d, lda, b_d, ldb, beta, c_d, ldc);
  cudaMemcpy(c, c_d, ldc * n * sizeof(double), cudaMemcpyDeviceToHost);
  cudaFree(a_d);
  cudaFree(b_d);
  cudaFree(c_d);
}

--------------- Code Block 7 ---------------
__global__ void gemm_kernel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  double temp = 0.0;

  if (i < m && j < n) {
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = (m + blockSize - 1) / blockSize;
  dim3 blockDim(blockSize, blockSize);
  dim3 gridDim(numBlocks, numBlocks);

  double *a_d, *b_d, *c_d;
  cudaMalloc((void **)&a_d, lda * k * sizeof(double));
  cudaMalloc((void **)&b_d, ldb * n * sizeof(double));
  cudaMalloc((void **)&c_d, ldc * n * sizeof(double));

  cudaMemcpy(a_d, a, lda * k * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(b_d, b, ldb * n * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(c_d, c, ldc * n * sizeof(double), cudaMemcpyHostToDevice);

  gemm_kernel<<<gridDim, blockDim>>>(m, n, k, alpha, a_d, lda, b_d, ldb, beta, c_d, ldc);

  cudaMemcpy(c, c_d, ldc * n * sizeof(double), cudaMemcpyDeviceToHost);

  cudaFree(a_d);
  cudaFree(b_d);
  cudaFree(c_d);
}

--------------- Code Block 8 ---------------
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  double temp = 0.0;

  if (i < m && j < n) {
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

void gemm_parallel_host(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = (m + blockSize - 1) / blockSize;
  dim3 dimBlock(blockSize, blockSize);
  dim3 dimGrid(numBlocks, numBlocks);

  double *a_d, *b_d, *c_d;
  cudaMalloc((void **)&a_d, lda * k * sizeof(double));
  cudaMalloc((void **)&b_d, ldb * n * sizeof(double));
  cudaMalloc((void **)&c_d, ldc * n * sizeof(double));

  cudaMemcpy(a_d, a, lda * k * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(b_d, b, ldb * n * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(c_d, c, ldc * n * sizeof(double), cudaMemcpyHostToDevice);

  gemm_parallel<<<dimGrid, dimBlock>>>(m, n, k, alpha, a_d, lda, b_d, ldb, beta, c_d, ldc);

  cudaMemcpy(c, c_d, ldc * n * sizeof(double), cudaMemcpyDeviceToHost);

  cudaFree(a_d);
  cudaFree(b_d);
  cudaFree(c_d);
}

--------------- Code Block 9 ---------------
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  double temp = 0.0;

  if (i < m && j < n) {
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

void launch_gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = (m + blockSize - 1) / blockSize;
  dim3 dimBlock(blockSize, blockSize);
  dim3 dimGrid(numBlocks, numBlocks);

  gemm_parallel<<<dimGrid, dimBlock>>>(m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
  cudaDeviceSynchronize();
}

--------------- Code Block 10 ---------------
__global__ void gemm_parallel(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;
  double temp = 0.0;

  if (i < m && j < n) {
    for (int l = 0; l < k; l++) {
      temp += a[i * lda + l] * b[l * ldb + j];
    }
    c[i * ldc + j] = alpha * temp + beta * c[i * ldc + j];
  }
}

void gemm_parallel_wrapper(int m, int n, int k, double alpha, double *a, int lda, double *b, int ldb, double beta, double *c, int ldc) {
  int blockSize = 16;
  int numBlocks = (m + blockSize - 1) / blockSize;
  dim3 dimBlock(blockSize, blockSize);
  dim3 dimGrid(numBlocks, numBlocks);

  double *a_d, *b_d, *c_d;
  cudaMalloc((void **)&a_d, lda * k * sizeof(double));
  cudaMalloc((void **)&b_d, ldb * n * sizeof(double));
  cudaMalloc((void **)&c_d, ldc * n * sizeof(double));

  cudaMemcpy(a_d, a, lda * k * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(b_d, b, ldb * n * sizeof(double), cudaMemcpyHostToDevice);
  cudaMemcpy(c_d, c, ldc * n * sizeof(double), cudaMemcpyHostToDevice);

  gemm_parallel<<<dimGrid, dimBlock>>>(m, n, k, alpha, a_d, lda, b_d, ldb, beta, c_d, ldc);

  cudaMemcpy(c, c_d, ldc * n * sizeof(double), cudaMemcpyDeviceToHost);

  cudaFree(a_d);
  cudaFree(b_d);
  cudaFree(c_d);
}

