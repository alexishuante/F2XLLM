Code Block 1:
extern "C" void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);

    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(THREADS_PER_BLOCK, THREADS_PER_BLOCK, THREADS_PER_BLOCK);
    dim3 numBlocks((n + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK,
                   (n + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK,
                   (n + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK);

    for (int iter = 0; iter < niter; ++iter) {
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();

        // Swap pointers
        double* temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double *u, double *unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;  // Skip the first and last element
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        int idx = IDX(i, j, k, n);
        unew[idx] = 0.125 * (u[IDX(i-1, j, k, n)] + u[IDX(i+1, j, k, n)] +
                             u[IDX(i, j-1, k, n)] + u[IDX(i, j+1, k, n)] +
                             u[IDX(i, j, k-1, n)] + u[IDX(i, j, k+1, n)] +
                             u[idx]);
    }
}

Code Block 2:
extern "C" void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);
    
    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);
    
    dim3 blockSize(8, 8, 8);
    dim3 gridSize((n + blockSize.x - 1) / blockSize.x,
                  (n + blockSize.y - 1) / blockSize.y,
                  (n + blockSize.z - 1) / blockSize.z);

    for (int iter = 0; iter < niter; ++iter) {
        jacobi_kernel<<<gridSize, blockSize>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();
        cudaMemcpy(d_u, d_unew, size, cudaMemcpyDeviceToDevice);
    }

    cudaMemcpy(unew, d_unew, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);
    
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        unew[IDX(i, j, k, n)] = 0.125 * (u[IDX(i-1, j, k, n)] + u[IDX(i+1, j, k, n)] +
                                         u[IDX(i, j-1, k, n)] + u[IDX(i, j+1, k, n)] +
                                         u[IDX(i, j, k-1, n)] + u[IDX(i, j, k+1, n)] +
                                         u[IDX(i, j, k, n)]);
    }
}

Code Block 3:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);
    
    cudaMalloc((void**)&d_u, size);
    cudaMalloc((void**)&d_unew, size);
    
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);
    
    dim3 blockSize(8, 8, 8);  // You may need to adjust this depending on your GPU
    dim3 gridSize((n + blockSize.x - 1) / blockSize.x, (n + blockSize.y - 1) / blockSize.y, (n + blockSize.z - 1) / blockSize.z);
    
    for (int iter = 0; iter < niter; iter++) {
        jacobiKernel<<<gridSize, blockSize>>>(d_u, d_unew, n);
        cudaMemcpy(d_u, d_unew, size, cudaMemcpyDeviceToDevice);
    }
    
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);
    
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobiKernel(double* u, double* unew, int n) {
    int i = threadIdx.x + blockDim.x * blockIdx.x + 1;
    int j = threadIdx.y + blockDim.y * blockIdx.y + 1;
    int k = threadIdx.z + blockDim.z * blockIdx.z + 1;

    if (i < n - 1 && j < n - 1 && k < n - 1) {
        int idx = i + n * (j + n * k);
        unew[idx] = 0.125 * (u[(i-1) + n * (j + n * k)] + u[(i+1) + n * (j + n * k)] + 
                            u[i + n * (j-1 + n * k)] + u[i + n * (j+1 + n * k)] + 
                            u[i + n * (j + n * (k-1))] + u[i + n * (j + n * (k+1))] + 
                            u[idx]);
    }
}

Code Block 4:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
    size_t size = n * n * n * sizeof(double);
    double *d_u, *d_unew;

    cudaMalloc((void**)&d_u, size);
    cudaMalloc((void**)&d_unew, size);

    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);
    
    dim3 dimBlock(8, 8, 8);
    dim3 dimGrid((n+dimBlock.x-1)/dimBlock.x, (n+dimBlock.y-1)/dimBlock.y, (n+dimBlock.z-1)/dimBlock.z);

    for (int iter = 0; iter < niter; ++iter) {
        jacobi_kernel<<<dimGrid, dimBlock>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();

        // Swap pointers for next iteration
        double *temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double *u, double *unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 2;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 2;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 2;

    if (i >= 2 && i < n-1 && j >= 2 && j < n-1 && k >= 2 && k < n-1) {
        unew[IDX(i, j, k, n)] = 0.125 * (u[IDX(i-1, j, k, n)] +
                                         u[IDX(i+1, j, k, n)] +
                                         u[IDX(i, j-1, k, n)] +
                                         u[IDX(i, j+1, k, n)] +
                                         u[IDX(i, j, k-1, n)] +
                                         u[IDX(i, j, k+1, n)] +
                                         u[IDX(i, j, k, n)]);
    }
}

Code Block 5:
bounds
    if (i < n-1 && j < n-1 && k < n-1) {
        int idx = i + j * n + k * n * n;
        unew[idx] = 0.125 * (u[(i-1) + j * n + k * n * n] 
                           + u[(i+1) + j * n + k * n * n]
                           + u[i + (j-1) * n + k * n * n]
                           + u[i + (j+1) * n + k * n * n]
                           + u[i + j * n + (k-1) * n * n]
                           + u[i + j * n + (k+1) * n * n]
                           + u[idx]);
    }

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    // Calculate the indices of the current thread in the 3D grid
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    // Ensure we are within the bounds
    if (i < n-1 && j < n-1 && k < n-1) {
        int idx = i + j * n + k * n * n;
        unew[idx] = 0.125 * (u[(i-1) + j * n + k * n * n] 
                           + u[(i+1) + j * n + k * n * n]
                           + u[i + (j-1) * n + k * n * n]
                           + u[i + (j+1) * n + k * n * n]
                           + u[i + j * n + (k-1) * n * n]
                           + u[i + j * n + (k+1) * n * n]
                           + u[idx]);
    }
}

Code Block 6:
void jacobi_parallel(double *u, double *unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    // Allocate device memory
    cudaMalloc((void**)&d_u, size);
    cudaMalloc((void**)&d_unew, size);

    // Copy data from host to device
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    // Define grid and block dimensions
    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n + threadsPerBlock.x - 2) / threadsPerBlock.x,
                   (n + threadsPerBlock.y - 2) / threadsPerBlock.y,
                   (n + threadsPerBlock.z - 2) / threadsPerBlock.z);

    for (int iter = 0; iter < niter; ++iter) {
        // Launch kernel
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);

        // Swap d_u and d_unew for next iteration
        double *temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    // Copy data back from device to host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double *u, double *unew, int n) {
    int i = blockDim.x * blockIdx.x + threadIdx.x + 1;
    int j = blockDim.y * blockIdx.y + threadIdx.y + 1;
    int k = blockDim.z * blockIdx.z + threadIdx.z + 1;

    if (i < n - 1 && j < n - 1 && k < n - 1) {
        unew[IDX(i, j, k, n)] = 0.125 * (u[IDX(i-1, j, k, n)] + u[IDX(i+1, j, k, n)] +
                                          u[IDX(i, j-1, k, n)] + u[IDX(i, j+1, k, n)] +
                                          u[IDX(i, j, k-1, n)] + u[IDX(i, j, k+1, n)] +
                                          u[IDX(i, j, k, n)]);
    }
}

Code Block 7:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);

    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);

    dim3 blockDim(8, 8, 8);
    dim3 gridDim((n+blockDim.x-1)/blockDim.x, (n+blockDim.y-1)/blockDim.y, (n+blockDim.z-1)/blockDim.z);

    for (int iter = 0; iter < niter; iter++) {
        jacobiKernel<<<gridDim, blockDim>>>(d_u, d_unew, n);
        cudaDeviceSynchronize();

        double *temp = d_u;
        d_u = d_unew;
        d_unew = temp;
    }

    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobiKernel(double* u, double* unew, int n) {
    int i = threadIdx.x + blockIdx.x * blockDim.x + 1;
    int j = threadIdx.y + blockIdx.y * blockDim.y + 1;
    int k = threadIdx.z + blockIdx.z * blockDim.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        int idx = i + j * n + k * n * n;
        int idx_im1 = (i-1) + j * n + k * n * n;
        int idx_ip1 = (i+1) + j * n + k * n * n;
        int idx_jm1 = i + (j-1) * n + k * n * n;
        int idx_jp1 = i + (j+1) * n + k * n * n;
        int idx_km1 = i + j * n + (k-1) * n * n;
        int idx_kp1 = i + j * n + (k+1) * n * n;
        
        unew[idx] = 0.125 * (u[idx_im1] + u[idx_ip1] + u[idx_jm1] + u[idx_jp1] + u[idx_km1] + u[idx_kp1] + u[idx]);
    }
}

Code Block 8:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);

    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n + threadsPerBlock.x - 1) / threadsPerBlock.x, 
                   (n + threadsPerBlock.y - 1) / threadsPerBlock.y,
                   (n + threadsPerBlock.z - 1) / threadsPerBlock.z);

    for (int iter = 0; iter < niter; ++iter) {
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);
        cudaMemcpy(d_u, d_unew, size, cudaMemcpyDeviceToDevice);
    }

    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n - 1 && j < n - 1 && k < n - 1) {
        int idx = i + j * n + k * n * n;
        unew[idx] = 0.125 * (u[(i-1) + j * n + k * n * n] + u[(i+1) + j * n + k * n * n] +
                             u[i + (j-1) * n + k * n * n] + u[i + (j+1) * n + k * n * n] +
                             u[i + j * n + (k-1) * n * n] + u[i + j * n + (k+1) * n * n] +
                             u[i + j * n + k * n * n]);
    }
}

Code Block 9:
void jacobi_parallel(double* u, double* unew, int n, int niter) {
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    // Allocate memory on the device
    cudaMalloc((void**)&d_u, size);
    cudaMalloc((void**)&d_unew, size);

    // Copy the data from host to device
    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    dim3 blockDims(8, 8, 8);
    dim3 gridDims((n + blockDims.x - 1) / blockDims.x,
                  (n + blockDims.y - 1) / blockDims.y,
                  (n + blockDims.z - 1) / blockDims.z);

    for (int iter = 0; iter < niter; iter++) {
        jacobi_kernel<<<gridDims, blockDims>>>(d_u, d_unew, n);
        cudaMemcpy(d_u, d_unew, size, cudaMemcpyDeviceToDevice);
    }

    // Copy the result back to the host
    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double* u, double* unew, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        unew[i + n * (j + n * k)] = 0.125 * (u[(i-1) + n * (j + n * k)] +
                                             u[(i+1) + n * (j + n * k)] +
                                             u[i + n * ((j-1) + n * k)] +
                                             u[i + n * ((j+1) + n * k)] +
                                             u[i + n * (j + n * (k-1))] +
                                             u[i + n * (j + n * (k+1))] +
                                             u[i + n * (j + n * k)]);
    }
}

Code Block 10:
void jacobi_parallel(double *u, double *unew, int n, int niter)
{
    double *d_u, *d_unew;
    size_t size = n * n * n * sizeof(double);

    cudaMalloc(&d_u, size);
    cudaMalloc(&d_unew, size);

    cudaMemcpy(d_u, u, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_unew, unew, size, cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(8, 8, 8);
    dim3 numBlocks((n + threadsPerBlock.x - 3) / threadsPerBlock.x,
                   (n + threadsPerBlock.y - 3) / threadsPerBlock.y,
                   (n + threadsPerBlock.z - 3) / threadsPerBlock.z);

    for (int iter = 0; iter < niter; iter++) {
        jacobi_kernel<<<numBlocks, threadsPerBlock>>>(d_u, d_unew, n);
        cudaMemcpy(d_u, d_unew, size, cudaMemcpyDeviceToDevice);
    }

    cudaMemcpy(u, d_u, size, cudaMemcpyDeviceToHost);

    cudaFree(d_u);
    cudaFree(d_unew);
}

__global__ void jacobi_kernel(double *u, double *unew, int n)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x + 1;
    int j = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int k = blockIdx.z * blockDim.z + threadIdx.z + 1;

    if (i < n-1 && j < n-1 && k < n-1) {
        unew[IDX(i, j, k, n)] = 0.125 * (
            u[IDX(i-1, j, k, n)] + u[IDX(i+1, j, k, n)] +
            u[IDX(i, j-1, k, n)] + u[IDX(i, j+1, k, n)] +
            u[IDX(i, j, k-1, n)] + u[IDX(i, j, k+1, n)] +
            u[IDX(i, j, k, n)]
        );
    }
}

